/work/09894/nicolas_/ls6/rl-final-project
Sat Apr 27 19:53:59 CDT 2024
/work/09894/nicolas_/ls6/miniconda3/envs/rl-final/bin:/work/09894/nicolas_/ls6/miniconda3/condabin:/work/09894/nicolas_/ls6/rl-final-project/google-cloud-sdk/bin:/opt/apps/xalt/xalt/bin:/opt/apps/pmix/3.2.3/bin:/opt/apps/cmake/3.24.2/bin:/opt/apps/intel19/python3/3.9.7/bin:/opt/apps/autotools/1.4/bin:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2020.1.217/linux/bin/intel64:/opt/apps/gcc/9.4.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.
========== multitask-atari ==========
Seed: 3434305822
Loading hyperparameters from: /work/09894/nicolas_/ls6/miniconda3/envs/rl-final/lib/python3.10/site-packages/rl_zoo3/hyperparams/ppo.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('batch_size', 8),
             ('clip_range', 0.4),
             ('ent_coef', 7.471297410891365e-08),
             ('gae_lambda', 1.0),
             ('gamma', 0.95),
             ('learning_rate', 4.551835986821376e-05),
             ('max_grad_norm', 1),
             ('n_epochs', 20),
             ('n_steps', 32),
             ('n_timesteps', 3600),
             ('policy', 'CnnPolicy'),
             ('vf_coef', 0.6098449016363978)])
Using 1 environments
Doing 1 intermediate evaluations for pruning based on the number of timesteps. (1 evaluation every 100k timesteps)
Optimizing hyperparameters
Sampler: tpe - Pruner: median
Number of finished trials:  500
Best trial:
Value:  -1.6
Params: 
    batch_size: 32
    n_steps: 512
    gamma: 0.95
    learning_rate: 0.002045382183858041
    ent_coef: 0.008834502476549207
    clip_range: 0.3
    n_epochs: 10
    gae_lambda: 0.95
    max_grad_norm: 5
    vf_coef: 0.4675624325799147
    net_arch: small
    activation_fn: relu
Writing report to logs/ppo/report_multitask-atari_500-trials-3600-tpe-median_1714275685
