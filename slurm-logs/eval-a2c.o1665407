/work/09894/nicolas_/ls6/rl-final-project
Sun Apr 28 18:35:54 CDT 2024
/work/09894/nicolas_/ls6/miniconda3/envs/rl-final/bin:/work/09894/nicolas_/ls6/miniconda3/condabin:/work/09894/nicolas_/ls6/rl-final-project/google-cloud-sdk/bin:/opt/apps/xalt/xalt/bin:/opt/apps/pmix/3.2.3/bin:/opt/apps/cmake/3.24.2/bin:/opt/apps/intel19/python3/3.9.7/bin:/opt/apps/autotools/1.4/bin:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2020.1.217/linux/bin/intel64:/opt/apps/gcc/9.4.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.
========== multitask-atari ==========
Seed: 3445177527
Loading hyperparameters from: /work/09894/nicolas_/ls6/miniconda3/envs/rl-final/lib/python3.10/site-packages/rl_zoo3/hyperparams/a2c.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('ent_coef', 2.4772847159542594e-06),
             ('gae_lambda', 0.8),
             ('gamma', 0.95),
             ('learning_rate', 0.2639483765900524),
             ('max_grad_norm', 1),
             ('n_steps', 256),
             ('n_timesteps', 10000000.0),
             ('normalize_advantage', False),
             ('policy', 'CnnPolicy'),
             ('policy_kwargs',
              'dict(ortho_init=False, '
              'net_arch=dict(pi=[256,256],vf=[256,256]), '
              'activation_fn=nn.ReLU)'),
             ('use_rms_prop', True),
             ('vf_coef', 0.1990617811319626)])
Using 1 environments
Creating test environment
Using cpu device
Log path: logs/a2c/multitask-atari_18
Eval num_timesteps=6000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 6000     |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.541    |
|    learning_rate      | 0.264    |
|    n_updates          | 23       |
|    policy_loss        | -0       |
|    value_loss         | 2.38e+21 |
------------------------------------
New best mean reward!
Eval num_timesteps=12000, episode_reward=-5.20 +/- 8.84
Episode length: 348.80 +/- 206.01
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 12000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.606    |
|    learning_rate      | 0.264    |
|    n_updates          | 46       |
|    policy_loss        | -0       |
|    value_loss         | 2.36e+21 |
------------------------------------
Eval num_timesteps=18000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 18000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.27     |
|    learning_rate      | 0.264    |
|    n_updates          | 70       |
|    policy_loss        | -0       |
|    value_loss         | 2.48e+21 |
------------------------------------
New best mean reward!
Eval num_timesteps=24000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 24000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.601    |
|    learning_rate      | 0.264    |
|    n_updates          | 93       |
|    policy_loss        | -0       |
|    value_loss         | 2.53e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 271      |
|    ep_rew_mean        | -2.19    |
| time/                 |          |
|    fps                | 292      |
|    iterations         | 100      |
|    time_elapsed       | 87       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.861    |
|    learning_rate      | 0.264    |
|    n_updates          | 99       |
|    policy_loss        | -0       |
|    value_loss         | 1.49e+21 |
------------------------------------
Eval num_timesteps=30000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 30000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.766    |
|    learning_rate      | 0.264    |
|    n_updates          | 117      |
|    policy_loss        | -0       |
|    value_loss         | 1.87e+21 |
------------------------------------
Eval num_timesteps=36000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 36000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.237    |
|    learning_rate      | 0.264    |
|    n_updates          | 140      |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+21 |
------------------------------------
New best mean reward!
Eval num_timesteps=42000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 42000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.762    |
|    learning_rate      | 0.264    |
|    n_updates          | 164      |
|    policy_loss        | -0       |
|    value_loss         | 1.91e+21 |
------------------------------------
Eval num_timesteps=48000, episode_reward=-5.20 +/- 8.82
Episode length: 348.60 +/- 205.27
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 48000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.539    |
|    learning_rate      | 0.264    |
|    n_updates          | 187      |
|    policy_loss        | -0       |
|    value_loss         | 2.23e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 276      |
|    ep_rew_mean        | -2.18    |
| time/                 |          |
|    fps                | 294      |
|    iterations         | 200      |
|    time_elapsed       | 173      |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0122  |
|    learning_rate      | 0.264    |
|    n_updates          | 199      |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=54000, episode_reward=-1.40 +/- 7.34
Episode length: 271.00 +/- 166.10
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 54000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.234    |
|    learning_rate      | 0.264    |
|    n_updates          | 210      |
|    policy_loss        | -0       |
|    value_loss         | 2.48e+21 |
------------------------------------
Eval num_timesteps=60000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 60000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.602    |
|    learning_rate      | 0.264    |
|    n_updates          | 234      |
|    policy_loss        | -0       |
|    value_loss         | 2.17e+21 |
------------------------------------
Eval num_timesteps=66000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 66000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.579    |
|    learning_rate      | 0.264    |
|    n_updates          | 257      |
|    policy_loss        | -0       |
|    value_loss         | 2.25e+21 |
------------------------------------
Eval num_timesteps=72000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 72000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.727    |
|    learning_rate      | 0.264    |
|    n_updates          | 281      |
|    policy_loss        | -0       |
|    value_loss         | 2.38e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 281      |
|    ep_rew_mean        | -2.04    |
| time/                 |          |
|    fps                | 296      |
|    iterations         | 300      |
|    time_elapsed       | 258      |
|    total_timesteps    | 76800    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.805    |
|    learning_rate      | 0.264    |
|    n_updates          | 299      |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
Eval num_timesteps=78000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 78000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.278   |
|    learning_rate      | 0.264    |
|    n_updates          | 304      |
|    policy_loss        | -0       |
|    value_loss         | 2.51e+21 |
------------------------------------
Eval num_timesteps=84000, episode_reward=-5.20 +/- 8.82
Episode length: 348.60 +/- 205.27
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 84000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.574    |
|    learning_rate      | 0.264    |
|    n_updates          | 328      |
|    policy_loss        | -0       |
|    value_loss         | 2.5e+21  |
------------------------------------
Eval num_timesteps=90000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 90000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.805    |
|    learning_rate      | 0.264    |
|    n_updates          | 351      |
|    policy_loss        | -0       |
|    value_loss         | 1.96e+21 |
------------------------------------
Eval num_timesteps=96000, episode_reward=-1.40 +/- 7.34
Episode length: 271.20 +/- 166.03
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 96000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.654    |
|    learning_rate      | 0.264    |
|    n_updates          | 374      |
|    policy_loss        | -0       |
|    value_loss         | 2.08e+21 |
------------------------------------
Eval num_timesteps=102000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 102000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.795    |
|    learning_rate      | 0.264    |
|    n_updates          | 398      |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 280      |
|    ep_rew_mean        | -2.06    |
| time/                 |          |
|    fps                | 295      |
|    iterations         | 400      |
|    time_elapsed       | 346      |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.867    |
|    learning_rate      | 0.264    |
|    n_updates          | 399      |
|    policy_loss        | -0       |
|    value_loss         | 1.17e+21 |
------------------------------------
Eval num_timesteps=108000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 108000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.441    |
|    learning_rate      | 0.264    |
|    n_updates          | 421      |
|    policy_loss        | -0       |
|    value_loss         | 2.02e+21 |
------------------------------------
Eval num_timesteps=114000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 114000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.464    |
|    learning_rate      | 0.264    |
|    n_updates          | 445      |
|    policy_loss        | -0       |
|    value_loss         | 2.69e+21 |
------------------------------------
Eval num_timesteps=120000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 120000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.54     |
|    learning_rate      | 0.264    |
|    n_updates          | 468      |
|    policy_loss        | -0       |
|    value_loss         | 2.24e+21 |
------------------------------------
Eval num_timesteps=126000, episode_reward=-4.80 +/- 9.15
Episode length: 361.00 +/- 195.47
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 361      |
|    mean_reward        | -4.8     |
| time/                 |          |
|    total_timesteps    | 126000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.762    |
|    learning_rate      | 0.264    |
|    n_updates          | 492      |
|    policy_loss        | -0       |
|    value_loss         | 1.84e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 282      |
|    ep_rew_mean        | -2.32    |
| time/                 |          |
|    fps                | 296      |
|    iterations         | 500      |
|    time_elapsed       | 431      |
|    total_timesteps    | 128000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0291  |
|    learning_rate      | 0.264    |
|    n_updates          | 499      |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=132000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 132000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.225    |
|    learning_rate      | 0.264    |
|    n_updates          | 515      |
|    policy_loss        | -0       |
|    value_loss         | 2.46e+21 |
------------------------------------
Eval num_timesteps=138000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 138000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.699    |
|    learning_rate      | 0.264    |
|    n_updates          | 539      |
|    policy_loss        | -0       |
|    value_loss         | 2.46e+21 |
------------------------------------
Eval num_timesteps=144000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 144000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.805    |
|    learning_rate      | 0.264    |
|    n_updates          | 562      |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
Eval num_timesteps=150000, episode_reward=-1.60 +/- 7.23
Episode length: 265.00 +/- 168.68
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 150000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.581    |
|    learning_rate      | 0.264    |
|    n_updates          | 585      |
|    policy_loss        | -0       |
|    value_loss         | 2.18e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 279      |
|    ep_rew_mean        | -2.09    |
| time/                 |          |
|    fps                | 297      |
|    iterations         | 600      |
|    time_elapsed       | 516      |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.599    |
|    learning_rate      | 0.264    |
|    n_updates          | 599      |
|    policy_loss        | -0       |
|    value_loss         | 2.2e+21  |
------------------------------------
Eval num_timesteps=156000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 156000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.44     |
|    learning_rate      | 0.264    |
|    n_updates          | 609      |
|    policy_loss        | -0       |
|    value_loss         | 2.73e+21 |
------------------------------------
Eval num_timesteps=162000, episode_reward=-5.20 +/- 8.84
Episode length: 348.60 +/- 206.20
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 162000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.588    |
|    learning_rate      | 0.264    |
|    n_updates          | 632      |
|    policy_loss        | -0       |
|    value_loss         | 2.27e+21 |
------------------------------------
Eval num_timesteps=168000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 168000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.616    |
|    learning_rate      | 0.264    |
|    n_updates          | 656      |
|    policy_loss        | -0       |
|    value_loss         | 2.22e+21 |
------------------------------------
Eval num_timesteps=174000, episode_reward=-2.00 +/- 7.04
Episode length: 252.40 +/- 175.45
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 174000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.806    |
|    learning_rate      | 0.264    |
|    n_updates          | 679      |
|    policy_loss        | -0       |
|    value_loss         | 1.96e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 279      |
|    ep_rew_mean        | -2.09    |
| time/                 |          |
|    fps                | 297      |
|    iterations         | 700      |
|    time_elapsed       | 601      |
|    total_timesteps    | 179200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.845    |
|    learning_rate      | 0.264    |
|    n_updates          | 699      |
|    policy_loss        | -0       |
|    value_loss         | 1.47e+21 |
------------------------------------
Eval num_timesteps=180000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 180000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.45     |
|    learning_rate      | 0.264    |
|    n_updates          | 703      |
|    policy_loss        | -0       |
|    value_loss         | 2.75e+21 |
------------------------------------
Eval num_timesteps=186000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 186000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.178   |
|    learning_rate      | 0.264    |
|    n_updates          | 726      |
|    policy_loss        | -0       |
|    value_loss         | 2.44e+21 |
------------------------------------
Eval num_timesteps=192000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 192000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.538    |
|    learning_rate      | 0.264    |
|    n_updates          | 749      |
|    policy_loss        | -0       |
|    value_loss         | 2.09e+21 |
------------------------------------
Eval num_timesteps=198000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 198000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.715    |
|    learning_rate      | 0.264    |
|    n_updates          | 773      |
|    policy_loss        | -0       |
|    value_loss         | 2.34e+21 |
------------------------------------
Eval num_timesteps=204000, episode_reward=-5.00 +/- 8.99
Episode length: 354.80 +/- 200.52
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 204000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.0514   |
|    learning_rate      | 0.264    |
|    n_updates          | 796      |
|    policy_loss        | -0       |
|    value_loss         | 2.42e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 275      |
|    ep_rew_mean        | -2.22    |
| time/                 |          |
|    fps                | 296      |
|    iterations         | 800      |
|    time_elapsed       | 689      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.0617   |
|    learning_rate      | 0.264    |
|    n_updates          | 799      |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=210000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 210000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.591    |
|    learning_rate      | 0.264    |
|    n_updates          | 820      |
|    policy_loss        | -0       |
|    value_loss         | 2.16e+21 |
------------------------------------
Eval num_timesteps=216000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 216000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.823    |
|    learning_rate      | 0.264    |
|    n_updates          | 843      |
|    policy_loss        | -0       |
|    value_loss         | 1.99e+21 |
------------------------------------
Eval num_timesteps=222000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 222000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.247    |
|    learning_rate      | 0.264    |
|    n_updates          | 867      |
|    policy_loss        | -0       |
|    value_loss         | 2.38e+21 |
------------------------------------
Eval num_timesteps=228000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 228000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.422    |
|    learning_rate      | 0.264    |
|    n_updates          | 890      |
|    policy_loss        | -0       |
|    value_loss         | 2.73e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 275      |
|    ep_rew_mean        | -2.21    |
| time/                 |          |
|    fps                | 297      |
|    iterations         | 900      |
|    time_elapsed       | 774      |
|    total_timesteps    | 230400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.432    |
|    learning_rate      | 0.264    |
|    n_updates          | 899      |
|    policy_loss        | -0       |
|    value_loss         | 2.32e+21 |
------------------------------------
Eval num_timesteps=234000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 234000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.794    |
|    learning_rate      | 0.264    |
|    n_updates          | 914      |
|    policy_loss        | -0       |
|    value_loss         | 1.91e+21 |
------------------------------------
Eval num_timesteps=240000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 240000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.714    |
|    learning_rate      | 0.264    |
|    n_updates          | 937      |
|    policy_loss        | -0       |
|    value_loss         | 2.33e+21 |
------------------------------------
Eval num_timesteps=246000, episode_reward=-5.40 +/- 8.66
Episode length: 342.40 +/- 210.63
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 342      |
|    mean_reward        | -5.4     |
| time/                 |          |
|    total_timesteps    | 246000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.688    |
|    learning_rate      | 0.264    |
|    n_updates          | 960      |
|    policy_loss        | -0       |
|    value_loss         | 2.01e+21 |
------------------------------------
Eval num_timesteps=252000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 252000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.576    |
|    learning_rate      | 0.264    |
|    n_updates          | 984      |
|    policy_loss        | -0       |
|    value_loss         | 2.05e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 278      |
|    ep_rew_mean        | -2.11    |
| time/                 |          |
|    fps                | 297      |
|    iterations         | 1000     |
|    time_elapsed       | 859      |
|    total_timesteps    | 256000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.852    |
|    learning_rate      | 0.264    |
|    n_updates          | 999      |
|    policy_loss        | -0       |
|    value_loss         | 1.7e+21  |
------------------------------------
Eval num_timesteps=258000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 258000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.69     |
|    learning_rate      | 0.264    |
|    n_updates          | 1007     |
|    policy_loss        | -0       |
|    value_loss         | 2.22e+21 |
------------------------------------
Eval num_timesteps=264000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 264000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.414    |
|    learning_rate      | 0.264    |
|    n_updates          | 1031     |
|    policy_loss        | -0       |
|    value_loss         | 2.73e+21 |
------------------------------------
Eval num_timesteps=270000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 270000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.649    |
|    learning_rate      | 0.264    |
|    n_updates          | 1054     |
|    policy_loss        | -0       |
|    value_loss         | 2.22e+21 |
------------------------------------
Eval num_timesteps=276000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 276000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.535    |
|    learning_rate      | 0.264    |
|    n_updates          | 1078     |
|    policy_loss        | -0       |
|    value_loss         | 2.09e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 275      |
|    ep_rew_mean        | -2.22    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 1100     |
|    time_elapsed       | 944      |
|    total_timesteps    | 281600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0304  |
|    learning_rate      | 0.264    |
|    n_updates          | 1099     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=282000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 282000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.577    |
|    learning_rate      | 0.264    |
|    n_updates          | 1101     |
|    policy_loss        | -0       |
|    value_loss         | 2.17e+21 |
------------------------------------
Eval num_timesteps=288000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 288000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.689   |
|    learning_rate      | 0.264    |
|    n_updates          | 1124     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+21 |
------------------------------------
Eval num_timesteps=294000, episode_reward=-5.20 +/- 8.82
Episode length: 348.60 +/- 205.27
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 294000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.72     |
|    learning_rate      | 0.264    |
|    n_updates          | 1148     |
|    policy_loss        | -0       |
|    value_loss         | 2.35e+21 |
------------------------------------
Eval num_timesteps=300000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 300000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.722    |
|    learning_rate      | 0.264    |
|    n_updates          | 1171     |
|    policy_loss        | -0       |
|    value_loss         | 2.12e+21 |
------------------------------------
Eval num_timesteps=306000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 306000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.805    |
|    learning_rate      | 0.264    |
|    n_updates          | 1195     |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 278      |
|    ep_rew_mean        | -2.12    |
| time/                 |          |
|    fps                | 297      |
|    iterations         | 1200     |
|    time_elapsed       | 1031     |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.72     |
|    learning_rate      | 0.264    |
|    n_updates          | 1199     |
|    policy_loss        | -0       |
|    value_loss         | 2.02e+21 |
------------------------------------
Eval num_timesteps=312000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 312000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.66     |
|    learning_rate      | 0.264    |
|    n_updates          | 1218     |
|    policy_loss        | -0       |
|    value_loss         | 2.26e+21 |
------------------------------------
Eval num_timesteps=318000, episode_reward=-1.80 +/- 7.11
Episode length: 258.80 +/- 171.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 318000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.762    |
|    learning_rate      | 0.264    |
|    n_updates          | 1242     |
|    policy_loss        | -0       |
|    value_loss         | 1.84e+21 |
------------------------------------
Eval num_timesteps=324000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 324000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.596    |
|    learning_rate      | 0.264    |
|    n_updates          | 1265     |
|    policy_loss        | -0       |
|    value_loss         | 2.52e+21 |
------------------------------------
Eval num_timesteps=330000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 330000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.229    |
|    learning_rate      | 0.264    |
|    n_updates          | 1289     |
|    policy_loss        | -0       |
|    value_loss         | 2.47e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 270      |
|    ep_rew_mean        | -2.05    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 1300     |
|    time_elapsed       | 1116     |
|    total_timesteps    | 332800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.864    |
|    learning_rate      | 0.264    |
|    n_updates          | 1299     |
|    policy_loss        | -0       |
|    value_loss         | 1.66e+21 |
------------------------------------
Eval num_timesteps=336000, episode_reward=-5.00 +/- 8.99
Episode length: 354.80 +/- 200.52
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 336000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.795    |
|    learning_rate      | 0.264    |
|    n_updates          | 1312     |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
Eval num_timesteps=342000, episode_reward=-1.60 +/- 7.23
Episode length: 265.00 +/- 168.68
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 342000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.364    |
|    learning_rate      | 0.264    |
|    n_updates          | 1335     |
|    policy_loss        | -0       |
|    value_loss         | 2.28e+21 |
------------------------------------
Eval num_timesteps=348000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 348000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.605    |
|    learning_rate      | 0.264    |
|    n_updates          | 1359     |
|    policy_loss        | -0       |
|    value_loss         | 2.2e+21  |
------------------------------------
Eval num_timesteps=354000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 354000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.584    |
|    learning_rate      | 0.264    |
|    n_updates          | 1382     |
|    policy_loss        | -0       |
|    value_loss         | 2.26e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 284      |
|    ep_rew_mean        | -2.25    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 1400     |
|    time_elapsed       | 1201     |
|    total_timesteps    | 358400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0538  |
|    learning_rate      | 0.264    |
|    n_updates          | 1399     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=360000, episode_reward=-1.60 +/- 7.26
Episode length: 264.80 +/- 169.88
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 360000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.424    |
|    learning_rate      | 0.264    |
|    n_updates          | 1406     |
|    policy_loss        | -0       |
|    value_loss         | 2.76e+21 |
------------------------------------
Eval num_timesteps=366000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 366000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.417    |
|    learning_rate      | 0.264    |
|    n_updates          | 1429     |
|    policy_loss        | -0       |
|    value_loss         | 2.66e+21 |
------------------------------------
Eval num_timesteps=372000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 372000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.603    |
|    learning_rate      | 0.264    |
|    n_updates          | 1453     |
|    policy_loss        | -0       |
|    value_loss         | 2.57e+21 |
------------------------------------
Eval num_timesteps=378000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 378000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.424    |
|    learning_rate      | 0.264    |
|    n_updates          | 1476     |
|    policy_loss        | -0       |
|    value_loss         | 2.76e+21 |
------------------------------------
Eval num_timesteps=384000, episode_reward=-5.20 +/- 8.82
Episode length: 348.60 +/- 205.27
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 384000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.726    |
|    learning_rate      | 0.264    |
|    n_updates          | 1499     |
|    policy_loss        | -0       |
|    value_loss         | 2e+21    |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 277      |
|    ep_rew_mean     | -2.15    |
| time/              |          |
|    fps             | 297      |
|    iterations      | 1500     |
|    time_elapsed    | 1289     |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 390000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.239    |
|    learning_rate      | 0.264    |
|    n_updates          | 1523     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+21 |
------------------------------------
Eval num_timesteps=396000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 396000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.547    |
|    learning_rate      | 0.264    |
|    n_updates          | 1546     |
|    policy_loss        | -0       |
|    value_loss         | 2.39e+21 |
------------------------------------
Eval num_timesteps=402000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 402000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.63     |
|    learning_rate      | 0.264    |
|    n_updates          | 1570     |
|    policy_loss        | -0       |
|    value_loss         | 2.29e+21 |
------------------------------------
Eval num_timesteps=408000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 408000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.655    |
|    learning_rate      | 0.264    |
|    n_updates          | 1593     |
|    policy_loss        | -0       |
|    value_loss         | 1.85e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 280      |
|    ep_rew_mean        | -2.07    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 1600     |
|    time_elapsed       | 1373     |
|    total_timesteps    | 409600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.574    |
|    learning_rate      | 0.264    |
|    n_updates          | 1599     |
|    policy_loss        | -0       |
|    value_loss         | 2.59e+21 |
------------------------------------
Eval num_timesteps=414000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 414000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.528    |
|    learning_rate      | 0.264    |
|    n_updates          | 1617     |
|    policy_loss        | -0       |
|    value_loss         | 2.14e+21 |
------------------------------------
Eval num_timesteps=420000, episode_reward=-1.80 +/- 7.14
Episode length: 258.80 +/- 172.19
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 420000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.628    |
|    learning_rate      | 0.264    |
|    n_updates          | 1640     |
|    policy_loss        | -0       |
|    value_loss         | 2.29e+21 |
------------------------------------
Eval num_timesteps=426000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 426000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.421    |
|    learning_rate      | 0.264    |
|    n_updates          | 1664     |
|    policy_loss        | -0       |
|    value_loss         | 2.64e+21 |
------------------------------------
Eval num_timesteps=432000, episode_reward=-5.20 +/- 8.82
Episode length: 348.60 +/- 205.27
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 432000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.61     |
|    learning_rate      | 0.264    |
|    n_updates          | 1687     |
|    policy_loss        | -0       |
|    value_loss         | 2.21e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 281      |
|    ep_rew_mean        | -2.33    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 1700     |
|    time_elapsed       | 1458     |
|    total_timesteps    | 435200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0226  |
|    learning_rate      | 0.264    |
|    n_updates          | 1699     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=438000, episode_reward=-1.80 +/- 7.14
Episode length: 258.80 +/- 172.19
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 438000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.603    |
|    learning_rate      | 0.264    |
|    n_updates          | 1710     |
|    policy_loss        | -0       |
|    value_loss         | 2.18e+21 |
------------------------------------
Eval num_timesteps=444000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 444000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.584    |
|    learning_rate      | 0.264    |
|    n_updates          | 1734     |
|    policy_loss        | -0       |
|    value_loss         | 2.26e+21 |
------------------------------------
Eval num_timesteps=450000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 450000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.257    |
|    learning_rate      | 0.264    |
|    n_updates          | 1757     |
|    policy_loss        | -0       |
|    value_loss         | 2.49e+21 |
------------------------------------
Eval num_timesteps=456000, episode_reward=-1.80 +/- 7.11
Episode length: 258.80 +/- 171.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 456000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.584    |
|    learning_rate      | 0.264    |
|    n_updates          | 1781     |
|    policy_loss        | -0       |
|    value_loss         | 2.26e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 276      |
|    ep_rew_mean        | -2.19    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 1800     |
|    time_elapsed       | 1543     |
|    total_timesteps    | 460800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.58     |
|    learning_rate      | 0.264    |
|    n_updates          | 1799     |
|    policy_loss        | -0       |
|    value_loss         | 2.25e+21 |
------------------------------------
Eval num_timesteps=462000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 462000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.805    |
|    learning_rate      | 0.264    |
|    n_updates          | 1804     |
|    policy_loss        | -0       |
|    value_loss         | 1.98e+21 |
------------------------------------
Eval num_timesteps=468000, episode_reward=-5.00 +/- 8.99
Episode length: 354.80 +/- 200.52
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 468000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.536    |
|    learning_rate      | 0.264    |
|    n_updates          | 1828     |
|    policy_loss        | -0       |
|    value_loss         | 2.06e+21 |
------------------------------------
Eval num_timesteps=474000, episode_reward=-1.40 +/- 7.34
Episode length: 271.20 +/- 166.03
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 474000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.181    |
|    learning_rate      | 0.264    |
|    n_updates          | 1851     |
|    policy_loss        | -0       |
|    value_loss         | 2.81e+21 |
------------------------------------
Eval num_timesteps=480000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 480000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.593    |
|    learning_rate      | 0.264    |
|    n_updates          | 1874     |
|    policy_loss        | -0       |
|    value_loss         | 2.27e+21 |
------------------------------------
Eval num_timesteps=486000, episode_reward=-1.40 +/- 7.34
Episode length: 271.00 +/- 166.10
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 486000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.426    |
|    learning_rate      | 0.264    |
|    n_updates          | 1898     |
|    policy_loss        | -0       |
|    value_loss         | 2.71e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 281      |
|    ep_rew_mean        | -2.04    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 1900     |
|    time_elapsed       | 1630     |
|    total_timesteps    | 486400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.859    |
|    learning_rate      | 0.264    |
|    n_updates          | 1899     |
|    policy_loss        | -0       |
|    value_loss         | 1.43e+21 |
------------------------------------
Eval num_timesteps=492000, episode_reward=-1.00 +/- 7.51
Episode length: 283.40 +/- 158.75
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 283      |
|    mean_reward        | -1       |
| time/                 |          |
|    total_timesteps    | 492000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.586    |
|    learning_rate      | 0.264    |
|    n_updates          | 1921     |
|    policy_loss        | -0       |
|    value_loss         | 2.19e+21 |
------------------------------------
New best mean reward!
Eval num_timesteps=498000, episode_reward=-1.40 +/- 7.34
Episode length: 271.00 +/- 166.10
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 498000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.568    |
|    learning_rate      | 0.264    |
|    n_updates          | 1945     |
|    policy_loss        | -0       |
|    value_loss         | 2.39e+21 |
------------------------------------
Eval num_timesteps=504000, episode_reward=-5.40 +/- 8.66
Episode length: 342.40 +/- 210.63
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 342      |
|    mean_reward        | -5.4     |
| time/                 |          |
|    total_timesteps    | 504000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.585    |
|    learning_rate      | 0.264    |
|    n_updates          | 1968     |
|    policy_loss        | -0       |
|    value_loss         | 2.19e+21 |
------------------------------------
Eval num_timesteps=510000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 510000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.602    |
|    learning_rate      | 0.264    |
|    n_updates          | 1992     |
|    policy_loss        | -0       |
|    value_loss         | 2.59e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 282      |
|    ep_rew_mean        | -2.29    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2000     |
|    time_elapsed       | 1716     |
|    total_timesteps    | 512000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.0622   |
|    learning_rate      | 0.264    |
|    n_updates          | 1999     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=516000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 516000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.219    |
|    learning_rate      | 0.264    |
|    n_updates          | 2015     |
|    policy_loss        | -0       |
|    value_loss         | 2.44e+21 |
------------------------------------
Eval num_timesteps=522000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 522000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.271    |
|    learning_rate      | 0.264    |
|    n_updates          | 2039     |
|    policy_loss        | -0       |
|    value_loss         | 2.34e+21 |
------------------------------------
Eval num_timesteps=528000, episode_reward=-2.00 +/- 7.04
Episode length: 252.60 +/- 175.33
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 253      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 528000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.225    |
|    learning_rate      | 0.264    |
|    n_updates          | 2062     |
|    policy_loss        | -0       |
|    value_loss         | 2.41e+21 |
------------------------------------
Eval num_timesteps=534000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 534000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.583    |
|    learning_rate      | 0.264    |
|    n_updates          | 2085     |
|    policy_loss        | -0       |
|    value_loss         | 2.2e+21  |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 275      |
|    ep_rew_mean        | -2.21    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2100     |
|    time_elapsed       | 1800     |
|    total_timesteps    | 537600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.635    |
|    learning_rate      | 0.264    |
|    n_updates          | 2099     |
|    policy_loss        | -0       |
|    value_loss         | 2.24e+21 |
------------------------------------
Eval num_timesteps=540000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 540000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.603    |
|    learning_rate      | 0.264    |
|    n_updates          | 2109     |
|    policy_loss        | -0       |
|    value_loss         | 2.57e+21 |
------------------------------------
Eval num_timesteps=546000, episode_reward=-5.00 +/- 9.01
Episode length: 354.80 +/- 201.48
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 546000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.411    |
|    learning_rate      | 0.264    |
|    n_updates          | 2132     |
|    policy_loss        | -0       |
|    value_loss         | 2.48e+21 |
------------------------------------
Eval num_timesteps=552000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 552000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.821    |
|    learning_rate      | 0.264    |
|    n_updates          | 2156     |
|    policy_loss        | -0       |
|    value_loss         | 1.95e+21 |
------------------------------------
Eval num_timesteps=558000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 558000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.757    |
|    learning_rate      | 0.264    |
|    n_updates          | 2179     |
|    policy_loss        | -0       |
|    value_loss         | 1.89e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 269      |
|    ep_rew_mean        | -2.09    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2200     |
|    time_elapsed       | 1886     |
|    total_timesteps    | 563200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.573    |
|    learning_rate      | 0.264    |
|    n_updates          | 2199     |
|    policy_loss        | -0       |
|    value_loss         | 2.24e+21 |
------------------------------------
Eval num_timesteps=564000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 564000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.761    |
|    learning_rate      | 0.264    |
|    n_updates          | 2203     |
|    policy_loss        | -0       |
|    value_loss         | 1.84e+21 |
------------------------------------
Eval num_timesteps=570000, episode_reward=-1.40 +/- 7.34
Episode length: 271.20 +/- 165.95
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 570000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.57     |
|    learning_rate      | 0.264    |
|    n_updates          | 2226     |
|    policy_loss        | -0       |
|    value_loss         | 2.5e+21  |
------------------------------------
Eval num_timesteps=576000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 576000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.537    |
|    learning_rate      | 0.264    |
|    n_updates          | 2249     |
|    policy_loss        | -0       |
|    value_loss         | 2.22e+21 |
------------------------------------
Eval num_timesteps=582000, episode_reward=-5.20 +/- 8.84
Episode length: 348.80 +/- 206.01
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 582000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.55     |
|    learning_rate      | 0.264    |
|    n_updates          | 2273     |
|    policy_loss        | -0       |
|    value_loss         | 2.39e+21 |
------------------------------------
Eval num_timesteps=588000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 588000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.212    |
|    learning_rate      | 0.264    |
|    n_updates          | 2296     |
|    policy_loss        | -0       |
|    value_loss         | 2.43e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 283      |
|    ep_rew_mean        | -2.28    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2300     |
|    time_elapsed       | 1973     |
|    total_timesteps    | 588800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.0618   |
|    learning_rate      | 0.264    |
|    n_updates          | 2299     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=594000, episode_reward=-1.60 +/- 7.23
Episode length: 265.00 +/- 168.61
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 594000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.805    |
|    learning_rate      | 0.264    |
|    n_updates          | 2320     |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
Eval num_timesteps=600000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 600000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.591    |
|    learning_rate      | 0.264    |
|    n_updates          | 2343     |
|    policy_loss        | -0       |
|    value_loss         | 2.27e+21 |
------------------------------------
Eval num_timesteps=606000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 606000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.762    |
|    learning_rate      | 0.264    |
|    n_updates          | 2367     |
|    policy_loss        | -0       |
|    value_loss         | 1.84e+21 |
------------------------------------
Eval num_timesteps=612000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 612000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.581    |
|    learning_rate      | 0.264    |
|    n_updates          | 2390     |
|    policy_loss        | -0       |
|    value_loss         | 2.25e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 276      |
|    ep_rew_mean        | -2.18    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2400     |
|    time_elapsed       | 2058     |
|    total_timesteps    | 614400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.209    |
|    learning_rate      | 0.264    |
|    n_updates          | 2399     |
|    policy_loss        | -0       |
|    value_loss         | 2.43e+21 |
------------------------------------
Eval num_timesteps=618000, episode_reward=-5.00 +/- 8.99
Episode length: 354.80 +/- 200.52
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 618000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.584    |
|    learning_rate      | 0.264    |
|    n_updates          | 2414     |
|    policy_loss        | -0       |
|    value_loss         | 2.52e+21 |
------------------------------------
Eval num_timesteps=624000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 624000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.823    |
|    learning_rate      | 0.264    |
|    n_updates          | 2437     |
|    policy_loss        | -0       |
|    value_loss         | 1.99e+21 |
------------------------------------
Eval num_timesteps=630000, episode_reward=-2.00 +/- 7.01
Episode length: 252.60 +/- 174.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 253      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 630000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.386    |
|    learning_rate      | 0.264    |
|    n_updates          | 2460     |
|    policy_loss        | -0       |
|    value_loss         | 2.34e+21 |
------------------------------------
Eval num_timesteps=636000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 636000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.54     |
|    learning_rate      | 0.264    |
|    n_updates          | 2484     |
|    policy_loss        | -0       |
|    value_loss         | 2.22e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 270      |
|    ep_rew_mean        | -2.07    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2500     |
|    time_elapsed       | 2143     |
|    total_timesteps    | 640000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.775    |
|    learning_rate      | 0.264    |
|    n_updates          | 2499     |
|    policy_loss        | -0       |
|    value_loss         | 2.35e+21 |
------------------------------------
Eval num_timesteps=642000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 642000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.277    |
|    learning_rate      | 0.264    |
|    n_updates          | 2507     |
|    policy_loss        | -0       |
|    value_loss         | 2.28e+21 |
------------------------------------
Eval num_timesteps=648000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 648000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.275    |
|    learning_rate      | 0.264    |
|    n_updates          | 2531     |
|    policy_loss        | -0       |
|    value_loss         | 2.27e+21 |
------------------------------------
Eval num_timesteps=654000, episode_reward=-5.20 +/- 8.82
Episode length: 348.60 +/- 205.27
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 654000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.831    |
|    learning_rate      | 0.264    |
|    n_updates          | 2554     |
|    policy_loss        | -0       |
|    value_loss         | 1.98e+21 |
------------------------------------
Eval num_timesteps=660000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 660000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.822    |
|    learning_rate      | 0.264    |
|    n_updates          | 2578     |
|    policy_loss        | -0       |
|    value_loss         | 1.99e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 283      |
|    ep_rew_mean        | -2.29    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2600     |
|    time_elapsed       | 2229     |
|    total_timesteps    | 665600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.0699   |
|    learning_rate      | 0.264    |
|    n_updates          | 2599     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=666000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 666000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.601    |
|    learning_rate      | 0.264    |
|    n_updates          | 2601     |
|    policy_loss        | -0       |
|    value_loss         | 2.19e+21 |
------------------------------------
Eval num_timesteps=672000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 672000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.69     |
|    learning_rate      | 0.264    |
|    n_updates          | 2624     |
|    policy_loss        | -0       |
|    value_loss         | 1.59e+21 |
------------------------------------
Eval num_timesteps=678000, episode_reward=-2.20 +/- 6.91
Episode length: 246.20 +/- 177.31
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 678000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.597    |
|    learning_rate      | 0.264    |
|    n_updates          | 2648     |
|    policy_loss        | -0       |
|    value_loss         | 2.51e+21 |
------------------------------------
Eval num_timesteps=684000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 684000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.819    |
|    learning_rate      | 0.264    |
|    n_updates          | 2671     |
|    policy_loss        | -0       |
|    value_loss         | 1.95e+21 |
------------------------------------
Eval num_timesteps=690000, episode_reward=-4.80 +/- 9.15
Episode length: 361.00 +/- 195.47
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 361      |
|    mean_reward        | -4.8     |
| time/                 |          |
|    total_timesteps    | 690000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.627    |
|    learning_rate      | 0.264    |
|    n_updates          | 2695     |
|    policy_loss        | -0       |
|    value_loss         | 2.23e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 279      |
|    ep_rew_mean        | -2.1     |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2700     |
|    time_elapsed       | 2317     |
|    total_timesteps    | 691200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.603    |
|    learning_rate      | 0.264    |
|    n_updates          | 2699     |
|    policy_loss        | -0       |
|    value_loss         | 2.17e+21 |
------------------------------------
Eval num_timesteps=696000, episode_reward=-2.00 +/- 7.04
Episode length: 252.40 +/- 175.45
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 696000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.437    |
|    learning_rate      | 0.264    |
|    n_updates          | 2718     |
|    policy_loss        | -0       |
|    value_loss         | 2.73e+21 |
------------------------------------
Eval num_timesteps=702000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 702000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.622    |
|    learning_rate      | 0.264    |
|    n_updates          | 2742     |
|    policy_loss        | -0       |
|    value_loss         | 2.23e+21 |
------------------------------------
Eval num_timesteps=708000, episode_reward=-1.60 +/- 7.23
Episode length: 265.00 +/- 168.68
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 708000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.25     |
|    learning_rate      | 0.264    |
|    n_updates          | 2765     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+21 |
------------------------------------
Eval num_timesteps=714000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 714000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.626    |
|    learning_rate      | 0.264    |
|    n_updates          | 2789     |
|    policy_loss        | -0       |
|    value_loss         | 2.48e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 276      |
|    ep_rew_mean        | -2.2     |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2800     |
|    time_elapsed       | 2401     |
|    total_timesteps    | 716800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.853    |
|    learning_rate      | 0.264    |
|    n_updates          | 2799     |
|    policy_loss        | -0       |
|    value_loss         | 1.32e+21 |
------------------------------------
Eval num_timesteps=720000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 720000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.437    |
|    learning_rate      | 0.264    |
|    n_updates          | 2812     |
|    policy_loss        | -0       |
|    value_loss         | 2.73e+21 |
------------------------------------
Eval num_timesteps=726000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 726000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.267    |
|    learning_rate      | 0.264    |
|    n_updates          | 2835     |
|    policy_loss        | -0       |
|    value_loss         | 2.48e+21 |
------------------------------------
Eval num_timesteps=732000, episode_reward=-5.00 +/- 9.01
Episode length: 355.00 +/- 201.34
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 732000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.579    |
|    learning_rate      | 0.264    |
|    n_updates          | 2859     |
|    policy_loss        | -0       |
|    value_loss         | 2.51e+21 |
------------------------------------
Eval num_timesteps=738000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 738000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.787    |
|    learning_rate      | 0.264    |
|    n_updates          | 2882     |
|    policy_loss        | -0       |
|    value_loss         | 1.91e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 274      |
|    ep_rew_mean        | -2.25    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 2900     |
|    time_elapsed       | 2486     |
|    total_timesteps    | 742400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.029   |
|    learning_rate      | 0.264    |
|    n_updates          | 2899     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=744000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 744000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.597    |
|    learning_rate      | 0.264    |
|    n_updates          | 2906     |
|    policy_loss        | -0       |
|    value_loss         | 2.19e+21 |
------------------------------------
Eval num_timesteps=750000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 750000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.814    |
|    learning_rate      | 0.264    |
|    n_updates          | 2929     |
|    policy_loss        | -0       |
|    value_loss         | 1.96e+21 |
------------------------------------
Eval num_timesteps=756000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 756000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.804    |
|    learning_rate      | 0.264    |
|    n_updates          | 2953     |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
Eval num_timesteps=762000, episode_reward=-1.60 +/- 7.23
Episode length: 265.00 +/- 168.68
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 762000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.245    |
|    learning_rate      | 0.264    |
|    n_updates          | 2976     |
|    policy_loss        | -0       |
|    value_loss         | 2.39e+21 |
------------------------------------
Eval num_timesteps=768000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 768000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.647    |
|    learning_rate      | 0.264    |
|    n_updates          | 2999     |
|    policy_loss        | -0       |
|    value_loss         | 2.31e+21 |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 281      |
|    ep_rew_mean     | -2.04    |
| time/              |          |
|    fps             | 298      |
|    iterations      | 3000     |
|    time_elapsed    | 2574     |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=774000, episode_reward=-5.00 +/- 8.99
Episode length: 354.80 +/- 200.52
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 774000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.564    |
|    learning_rate      | 0.264    |
|    n_updates          | 3023     |
|    policy_loss        | -0       |
|    value_loss         | 2.16e+21 |
------------------------------------
Eval num_timesteps=780000, episode_reward=-1.40 +/- 7.34
Episode length: 271.00 +/- 166.10
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 780000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.637    |
|    learning_rate      | 0.264    |
|    n_updates          | 3046     |
|    policy_loss        | -0       |
|    value_loss         | 2.32e+21 |
------------------------------------
Eval num_timesteps=786000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 786000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.806    |
|    learning_rate      | 0.264    |
|    n_updates          | 3070     |
|    policy_loss        | -0       |
|    value_loss         | 1.94e+21 |
------------------------------------
Eval num_timesteps=792000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 792000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.441    |
|    learning_rate      | 0.264    |
|    n_updates          | 3093     |
|    policy_loss        | -0       |
|    value_loss         | 2.02e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 278      |
|    ep_rew_mean        | -2.13    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3100     |
|    time_elapsed       | 2659     |
|    total_timesteps    | 793600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.723    |
|    learning_rate      | 0.264    |
|    n_updates          | 3099     |
|    policy_loss        | -0       |
|    value_loss         | 2.55e+21 |
------------------------------------
Eval num_timesteps=798000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 798000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.224    |
|    learning_rate      | 0.264    |
|    n_updates          | 3117     |
|    policy_loss        | -0       |
|    value_loss         | 2.45e+21 |
------------------------------------
Eval num_timesteps=804000, episode_reward=-1.40 +/- 7.34
Episode length: 271.00 +/- 166.10
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 804000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.41     |
|    learning_rate      | 0.264    |
|    n_updates          | 3140     |
|    policy_loss        | -0       |
|    value_loss         | 2.33e+21 |
------------------------------------
Eval num_timesteps=810000, episode_reward=-4.80 +/- 9.15
Episode length: 361.00 +/- 195.47
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 361      |
|    mean_reward        | -4.8     |
| time/                 |          |
|    total_timesteps    | 810000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.44     |
|    learning_rate      | 0.264    |
|    n_updates          | 3164     |
|    policy_loss        | -0       |
|    value_loss         | 2.73e+21 |
------------------------------------
Eval num_timesteps=816000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 816000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.703    |
|    learning_rate      | 0.264    |
|    n_updates          | 3187     |
|    policy_loss        | -0       |
|    value_loss         | 2.29e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 284      |
|    ep_rew_mean        | -2.25    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3200     |
|    time_elapsed       | 2745     |
|    total_timesteps    | 819200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.0699   |
|    learning_rate      | 0.264    |
|    n_updates          | 3199     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=822000, episode_reward=-1.60 +/- 7.20
Episode length: 264.80 +/- 167.60
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 822000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.539    |
|    learning_rate      | 0.264    |
|    n_updates          | 3210     |
|    policy_loss        | -0       |
|    value_loss         | 2.09e+21 |
------------------------------------
Eval num_timesteps=828000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 828000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.703    |
|    learning_rate      | 0.264    |
|    n_updates          | 3234     |
|    policy_loss        | -0       |
|    value_loss         | 2.11e+21 |
------------------------------------
Eval num_timesteps=834000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 834000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.643    |
|    learning_rate      | 0.264    |
|    n_updates          | 3257     |
|    policy_loss        | -0       |
|    value_loss         | 2.18e+21 |
------------------------------------
Eval num_timesteps=840000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 840000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.607    |
|    learning_rate      | 0.264    |
|    n_updates          | 3281     |
|    policy_loss        | -0       |
|    value_loss         | 2.18e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 279      |
|    ep_rew_mean        | -2.08    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3300     |
|    time_elapsed       | 2829     |
|    total_timesteps    | 844800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.81     |
|    learning_rate      | 0.264    |
|    n_updates          | 3299     |
|    policy_loss        | -0       |
|    value_loss         | 1.44e+21 |
------------------------------------
Eval num_timesteps=846000, episode_reward=-5.20 +/- 8.84
Episode length: 348.60 +/- 206.20
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 846000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.54     |
|    learning_rate      | 0.264    |
|    n_updates          | 3304     |
|    policy_loss        | -0       |
|    value_loss         | 2.18e+21 |
------------------------------------
Eval num_timesteps=852000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 852000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.59     |
|    learning_rate      | 0.264    |
|    n_updates          | 3328     |
|    policy_loss        | -0       |
|    value_loss         | 2.27e+21 |
------------------------------------
Eval num_timesteps=858000, episode_reward=-2.20 +/- 6.91
Episode length: 246.40 +/- 177.20
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 246      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 858000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.212    |
|    learning_rate      | 0.264    |
|    n_updates          | 3351     |
|    policy_loss        | -0       |
|    value_loss         | 2.43e+21 |
------------------------------------
Eval num_timesteps=864000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 864000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.409    |
|    learning_rate      | 0.264    |
|    n_updates          | 3374     |
|    policy_loss        | -0       |
|    value_loss         | 2.49e+21 |
------------------------------------
Eval num_timesteps=870000, episode_reward=-1.40 +/- 7.31
Episode length: 271.20 +/- 164.87
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 870000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.607    |
|    learning_rate      | 0.264    |
|    n_updates          | 3398     |
|    policy_loss        | -0       |
|    value_loss         | 2.3e+21  |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 276      |
|    ep_rew_mean        | -2.19    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3400     |
|    time_elapsed       | 2917     |
|    total_timesteps    | 870400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.561    |
|    learning_rate      | 0.264    |
|    n_updates          | 3399     |
|    policy_loss        | -0       |
|    value_loss         | 2.24e+21 |
------------------------------------
Eval num_timesteps=876000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 876000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.691    |
|    learning_rate      | 0.264    |
|    n_updates          | 3421     |
|    policy_loss        | -0       |
|    value_loss         | 2e+21    |
------------------------------------
Eval num_timesteps=882000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 882000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.828    |
|    learning_rate      | 0.264    |
|    n_updates          | 3445     |
|    policy_loss        | -0       |
|    value_loss         | 1.95e+21 |
------------------------------------
Eval num_timesteps=888000, episode_reward=-4.80 +/- 9.15
Episode length: 361.00 +/- 195.47
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 361      |
|    mean_reward        | -4.8     |
| time/                 |          |
|    total_timesteps    | 888000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.556    |
|    learning_rate      | 0.264    |
|    n_updates          | 3468     |
|    policy_loss        | -0       |
|    value_loss         | 2.17e+21 |
------------------------------------
Eval num_timesteps=894000, episode_reward=-2.20 +/- 6.91
Episode length: 246.60 +/- 177.09
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 247      |
|    mean_reward        | -2.2     |
| time/                 |          |
|    total_timesteps    | 894000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.642    |
|    learning_rate      | 0.264    |
|    n_updates          | 3492     |
|    policy_loss        | -0       |
|    value_loss         | 2.21e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 276      |
|    ep_rew_mean        | -2.19    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3500     |
|    time_elapsed       | 3003     |
|    total_timesteps    | 896000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0285  |
|    learning_rate      | 0.264    |
|    n_updates          | 3499     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=900000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 900000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.804    |
|    learning_rate      | 0.264    |
|    n_updates          | 3515     |
|    policy_loss        | -0       |
|    value_loss         | 1.96e+21 |
------------------------------------
Eval num_timesteps=906000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 906000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.22     |
|    learning_rate      | 0.264    |
|    n_updates          | 3539     |
|    policy_loss        | -0       |
|    value_loss         | 2.44e+21 |
------------------------------------
Eval num_timesteps=912000, episode_reward=-1.80 +/- 7.14
Episode length: 258.60 +/- 172.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 912000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.6      |
|    learning_rate      | 0.264    |
|    n_updates          | 3562     |
|    policy_loss        | -0       |
|    value_loss         | 2.29e+21 |
------------------------------------
Eval num_timesteps=918000, episode_reward=-1.00 +/- 7.51
Episode length: 283.40 +/- 158.75
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 283      |
|    mean_reward        | -1       |
| time/                 |          |
|    total_timesteps    | 918000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.672    |
|    learning_rate      | 0.264    |
|    n_updates          | 3585     |
|    policy_loss        | -0       |
|    value_loss         | 2.13e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 286      |
|    ep_rew_mean        | -2.17    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3600     |
|    time_elapsed       | 3088     |
|    total_timesteps    | 921600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.528    |
|    learning_rate      | 0.264    |
|    n_updates          | 3599     |
|    policy_loss        | -0       |
|    value_loss         | 2.15e+21 |
------------------------------------
Eval num_timesteps=924000, episode_reward=-5.00 +/- 8.99
Episode length: 354.80 +/- 200.52
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 924000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.708    |
|    learning_rate      | 0.264    |
|    n_updates          | 3609     |
|    policy_loss        | -0       |
|    value_loss         | 2.33e+21 |
------------------------------------
Eval num_timesteps=930000, episode_reward=-1.40 +/- 7.31
Episode length: 271.20 +/- 164.87
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 930000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.44     |
|    learning_rate      | 0.264    |
|    n_updates          | 3632     |
|    policy_loss        | -0       |
|    value_loss         | 2.73e+21 |
------------------------------------
Eval num_timesteps=936000, episode_reward=-2.00 +/- 7.01
Episode length: 252.60 +/- 174.24
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 253      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 936000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.66     |
|    learning_rate      | 0.264    |
|    n_updates          | 3656     |
|    policy_loss        | -0       |
|    value_loss         | 2.26e+21 |
------------------------------------
Eval num_timesteps=942000, episode_reward=-1.60 +/- 7.23
Episode length: 265.00 +/- 168.64
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 942000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.57     |
|    learning_rate      | 0.264    |
|    n_updates          | 3679     |
|    policy_loss        | -0       |
|    value_loss         | 2.13e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 277      |
|    ep_rew_mean        | -2.16    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3700     |
|    time_elapsed       | 3173     |
|    total_timesteps    | 947200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.46     |
|    learning_rate      | 0.264    |
|    n_updates          | 3699     |
|    policy_loss        | -0       |
|    value_loss         | 2.68e+21 |
------------------------------------
Eval num_timesteps=948000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 948000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.583    |
|    learning_rate      | 0.264    |
|    n_updates          | 3703     |
|    policy_loss        | -0       |
|    value_loss         | 2.54e+21 |
------------------------------------
Eval num_timesteps=954000, episode_reward=-1.40 +/- 7.34
Episode length: 271.20 +/- 166.03
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 954000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.533    |
|    learning_rate      | 0.264    |
|    n_updates          | 3726     |
|    policy_loss        | -0       |
|    value_loss         | 2.18e+21 |
------------------------------------
Eval num_timesteps=960000, episode_reward=-5.00 +/- 9.01
Episode length: 355.00 +/- 201.34
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 960000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.33     |
|    learning_rate      | 0.264    |
|    n_updates          | 3749     |
|    policy_loss        | -0       |
|    value_loss         | 2.31e+21 |
------------------------------------
Eval num_timesteps=966000, episode_reward=-2.00 +/- 7.01
Episode length: 252.40 +/- 174.35
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 252      |
|    mean_reward        | -2       |
| time/                 |          |
|    total_timesteps    | 966000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.62     |
|    learning_rate      | 0.264    |
|    n_updates          | 3773     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+21 |
------------------------------------
Eval num_timesteps=972000, episode_reward=-1.20 +/- 7.41
Episode length: 277.40 +/- 161.91
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 972000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.822    |
|    learning_rate      | 0.264    |
|    n_updates          | 3796     |
|    policy_loss        | -0       |
|    value_loss         | 1.99e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 277      |
|    ep_rew_mean        | -2.14    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3800     |
|    time_elapsed       | 3261     |
|    total_timesteps    | 972800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.0618   |
|    learning_rate      | 0.264    |
|    n_updates          | 3799     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=978000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 978000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.417    |
|    learning_rate      | 0.264    |
|    n_updates          | 3820     |
|    policy_loss        | -0       |
|    value_loss         | 2.66e+21 |
------------------------------------
Eval num_timesteps=984000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 984000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.534    |
|    learning_rate      | 0.264    |
|    n_updates          | 3843     |
|    policy_loss        | -0       |
|    value_loss         | 2.13e+21 |
------------------------------------
Eval num_timesteps=990000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 990000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.593    |
|    learning_rate      | 0.264    |
|    n_updates          | 3867     |
|    policy_loss        | -0       |
|    value_loss         | 2.28e+21 |
------------------------------------
Eval num_timesteps=996000, episode_reward=-5.00 +/- 9.01
Episode length: 354.80 +/- 201.48
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 355      |
|    mean_reward        | -5       |
| time/                 |          |
|    total_timesteps    | 996000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.569    |
|    learning_rate      | 0.264    |
|    n_updates          | 3890     |
|    policy_loss        | -0       |
|    value_loss         | 2.5e+21  |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 275      |
|    ep_rew_mean        | -2.21    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 3900     |
|    time_elapsed       | 3347     |
|    total_timesteps    | 998400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.538    |
|    learning_rate      | 0.264    |
|    n_updates          | 3899     |
|    policy_loss        | -0       |
|    value_loss         | 2.1e+21  |
------------------------------------
Eval num_timesteps=1002000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 1002000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.571    |
|    learning_rate      | 0.264    |
|    n_updates          | 3914     |
|    policy_loss        | -0       |
|    value_loss         | 2.45e+21 |
------------------------------------
Eval num_timesteps=1008000, episode_reward=-1.20 +/- 7.41
Episode length: 277.20 +/- 161.99
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 277      |
|    mean_reward        | -1.2     |
| time/                 |          |
|    total_timesteps    | 1008000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.601    |
|    learning_rate      | 0.264    |
|    n_updates          | 3937     |
|    policy_loss        | -0       |
|    value_loss         | 2.53e+21 |
------------------------------------
Eval num_timesteps=1014000, episode_reward=-1.60 +/- 7.23
Episode length: 264.80 +/- 168.74
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 1014000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.589    |
|    learning_rate      | 0.264    |
|    n_updates          | 3960     |
|    policy_loss        | -0       |
|    value_loss         | 2.18e+21 |
------------------------------------
Eval num_timesteps=1020000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 1020000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.553    |
|    learning_rate      | 0.264    |
|    n_updates          | 3984     |
|    policy_loss        | -0       |
|    value_loss         | 2.07e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 271      |
|    ep_rew_mean        | -2.04    |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 4000     |
|    time_elapsed       | 3432     |
|    total_timesteps    | 1024000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.748    |
|    learning_rate      | 0.264    |
|    n_updates          | 3999     |
|    policy_loss        | -0       |
|    value_loss         | 2.52e+21 |
------------------------------------
Eval num_timesteps=1026000, episode_reward=-1.80 +/- 7.14
Episode length: 258.80 +/- 172.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 1026000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.534    |
|    learning_rate      | 0.264    |
|    n_updates          | 4007     |
|    policy_loss        | -0       |
|    value_loss         | 2.16e+21 |
------------------------------------
Eval num_timesteps=1032000, episode_reward=-1.60 +/- 7.26
Episode length: 264.80 +/- 169.88
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 1032000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.765    |
|    learning_rate      | 0.264    |
|    n_updates          | 4031     |
|    policy_loss        | -0       |
|    value_loss         | 1.93e+21 |
------------------------------------
Eval num_timesteps=1038000, episode_reward=-5.20 +/- 8.84
Episode length: 348.60 +/- 206.20
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 349      |
|    mean_reward        | -5.2     |
| time/                 |          |
|    total_timesteps    | 1038000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.446    |
|    learning_rate      | 0.264    |
|    n_updates          | 4054     |
|    policy_loss        | -0       |
|    value_loss         | 2.74e+21 |
------------------------------------
Eval num_timesteps=1044000, episode_reward=-1.40 +/- 7.34
Episode length: 271.00 +/- 166.10
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 1044000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.649    |
|    learning_rate      | 0.264    |
|    n_updates          | 4078     |
|    policy_loss        | -0       |
|    value_loss         | 2.22e+21 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 272      |
|    ep_rew_mean        | -2.3     |
| time/                 |          |
|    fps                | 298      |
|    iterations         | 4100     |
|    time_elapsed       | 3518     |
|    total_timesteps    | 1049600  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0038  |
|    learning_rate      | 0.264    |
|    n_updates          | 4099     |
|    policy_loss        | -0       |
|    value_loss         | 1.2e+20  |
------------------------------------
Eval num_timesteps=1050000, episode_reward=-1.40 +/- 7.31
Episode length: 271.00 +/- 164.94
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 271      |
|    mean_reward        | -1.4     |
| time/                 |          |
|    total_timesteps    | 1050000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.229    |
|    learning_rate      | 0.264    |
|    n_updates          | 4101     |
|    policy_loss        | -0       |
|    value_loss         | 2.47e+21 |
------------------------------------
Eval num_timesteps=1056000, episode_reward=-1.80 +/- 7.11
Episode length: 258.60 +/- 171.12
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 259      |
|    mean_reward        | -1.8     |
| time/                 |          |
|    total_timesteps    | 1056000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.768    |
|    learning_rate      | 0.264    |
|    n_updates          | 4124     |
|    policy_loss        | -0       |
|    value_loss         | 1.93e+21 |
------------------------------------
Eval num_timesteps=1062000, episode_reward=-1.60 +/- 7.23
Episode length: 265.00 +/- 168.68
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 265      |
|    mean_reward        | -1.6     |
| time/                 |          |
|    total_timesteps    | 1062000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.579    |
|    learning_rate      | 0.264    |
|    n_updates          | 4148     |
|    policy_loss        | -0       |
|    value_loss         | 2.56e+21 |
------------------------------------
