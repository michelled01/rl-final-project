/work/09894/nicolas_/ls6/rl-final-project
Sun Apr 28 18:36:55 CDT 2024
/work/09894/nicolas_/ls6/miniconda3/envs/rl-final/bin:/work/09894/nicolas_/ls6/miniconda3/condabin:/work/09894/nicolas_/ls6/rl-final-project/google-cloud-sdk/bin:/opt/apps/xalt/xalt/bin:/opt/apps/pmix/3.2.3/bin:/opt/apps/cmake/3.24.2/bin:/opt/apps/intel19/python3/3.9.7/bin:/opt/apps/autotools/1.4/bin:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2020.1.217/linux/bin/intel64:/opt/apps/gcc/9.4.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.
========== multitask-atari ==========
Seed: 3027155368
Loading hyperparameters from: /work/09894/nicolas_/ls6/miniconda3/envs/rl-final/lib/python3.10/site-packages/rl_zoo3/hyperparams/ars.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('delta_std', 0.025),
             ('learning_rate', 0.01102734211262095),
             ('n_delta', 32),
             ('n_timesteps', 10000000.0),
             ('policy', 'MlpPolicy'),
             ('zero_policy', False)])
Using 1 environments
Creating test environment
Using cpu device
Log path: logs/ars/multitask-atari_11
Eval num_timesteps=6000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
New best mean reward!
Eval num_timesteps=12000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -9.6     |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
Eval num_timesteps=18000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
Eval num_timesteps=24000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -9.6     |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -6.02    |
|    return_std      | 8.38     |
| time/              |          |
|    fps             | 590      |
|    time_elapsed    | 52       |
|    total_timesteps | 31017    |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 0        |
|    learning_rate   | 0.011    |
|    step_size       | 4.11e-05 |
---------------------------------
Eval num_timesteps=36000, episode_reward=-7.60 +/- 6.37
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -7.6     |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=42000, episode_reward=-5.20 +/- 6.40
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
New best mean reward!
Eval num_timesteps=48000, episode_reward=-7.60 +/- 6.37
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -7.6     |
| time/              |          |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=54000, episode_reward=-6.00 +/- 7.38
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6       |
| time/              |          |
|    total_timesteps | 54000    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-7.00 +/- 6.20
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -7       |
| time/              |          |
|    total_timesteps | 60000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 498      |
|    ep_rew_mean     | -6.03    |
|    return_std      | 8.36     |
| time/              |          |
|    fps             | 595      |
|    time_elapsed    | 105      |
|    total_timesteps | 62918    |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 1        |
|    learning_rate   | 0.011    |
|    step_size       | 4.12e-05 |
---------------------------------
Eval num_timesteps=66000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 66000    |
---------------------------------
New best mean reward!
Eval num_timesteps=72000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 72000    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 78000    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 84000    |
---------------------------------
New best mean reward!
Eval num_timesteps=90000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 490      |
|    ep_rew_mean     | -6.23    |
|    return_std      | 8.46     |
| time/              |          |
|    fps             | 625      |
|    time_elapsed    | 150      |
|    total_timesteps | 94286    |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 2        |
|    learning_rate   | 0.011    |
|    step_size       | 4.07e-05 |
---------------------------------
Eval num_timesteps=96000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
Eval num_timesteps=102000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
Eval num_timesteps=108000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=114000, episode_reward=-1.20 +/- 7.41
Episode length: 297.00 +/- 153.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 474      |
|    ep_rew_mean     | -5.8     |
|    return_std      | 8.24     |
| time/              |          |
|    fps             | 641      |
|    time_elapsed    | 194      |
|    total_timesteps | 124643   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 3        |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=126000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=132000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=138000, episode_reward=-1.40 +/- 7.31
Episode length: 287.40 +/- 157.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 138000   |
---------------------------------
Eval num_timesteps=144000, episode_reward=-1.40 +/- 7.31
Episode length: 287.40 +/- 157.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 150000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 471      |
|    ep_rew_mean     | -5.91    |
|    return_std      | 8.18     |
| time/              |          |
|    fps             | 651      |
|    time_elapsed    | 237      |
|    total_timesteps | 154808   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 4        |
|    learning_rate   | 0.011    |
|    step_size       | 4.21e-05 |
---------------------------------
Eval num_timesteps=156000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 156000   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 162000   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 168000   |
---------------------------------
Eval num_timesteps=174000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 440      |
|    ep_rew_mean     | -5.23    |
|    return_std      | 8.17     |
| time/              |          |
|    fps             | 656      |
|    time_elapsed    | 278      |
|    total_timesteps | 182956   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 5        |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=186000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
Eval num_timesteps=192000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=198000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=204000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 500      |
|    ep_rew_mean     | -6.42    |
|    return_std      | 8.28     |
| time/              |          |
|    fps             | 663      |
|    time_elapsed    | 323      |
|    total_timesteps | 214953   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 6        |
|    learning_rate   | 0.011    |
|    step_size       | 4.16e-05 |
---------------------------------
Eval num_timesteps=216000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=222000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 222000   |
---------------------------------
Eval num_timesteps=228000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=234000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 234000   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 246000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 502      |
|    ep_rew_mean     | -6.31    |
|    return_std      | 8.21     |
| time/              |          |
|    fps             | 665      |
|    time_elapsed    | 371      |
|    total_timesteps | 247052   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 7        |
|    learning_rate   | 0.011    |
|    step_size       | 4.2e-05  |
---------------------------------
Eval num_timesteps=252000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 252000   |
---------------------------------
Eval num_timesteps=258000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
Eval num_timesteps=264000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
Eval num_timesteps=276000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 472      |
|    ep_rew_mean     | -5.84    |
|    return_std      | 8.23     |
| time/              |          |
|    fps             | 669      |
|    time_elapsed    | 414      |
|    total_timesteps | 277255   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 8        |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=282000, episode_reward=-4.80 +/- 9.15
Episode length: 377.60 +/- 182.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 378      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=288000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=294000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=306000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 306000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | -5.75    |
|    return_std      | 8.32     |
| time/              |          |
|    fps             | 670      |
|    time_elapsed    | 457      |
|    total_timesteps | 306767   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 9        |
|    learning_rate   | 0.011    |
|    step_size       | 4.14e-05 |
---------------------------------
Eval num_timesteps=312000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=318000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 318000   |
---------------------------------
Eval num_timesteps=324000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 324000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-1.20 +/- 7.41
Episode length: 297.20 +/- 152.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 330000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 438      |
|    ep_rew_mean     | -5.22    |
|    return_std      | 8.19     |
| time/              |          |
|    fps             | 674      |
|    time_elapsed    | 496      |
|    total_timesteps | 334810   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 10       |
|    learning_rate   | 0.011    |
|    step_size       | 4.21e-05 |
---------------------------------
Eval num_timesteps=336000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 336000   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
Eval num_timesteps=348000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
Eval num_timesteps=354000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -5.92    |
|    return_std      | 8.39     |
| time/              |          |
|    fps             | 676      |
|    time_elapsed    | 541      |
|    total_timesteps | 365834   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 11       |
|    learning_rate   | 0.011    |
|    step_size       | 4.11e-05 |
---------------------------------
Eval num_timesteps=366000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=372000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -9.6     |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=378000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=384000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -9.6     |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 390000   |
---------------------------------
Eval num_timesteps=396000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -9.6     |
| time/              |          |
|    total_timesteps | 396000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -6.08    |
|    return_std      | 8.31     |
| time/              |          |
|    fps             | 665      |
|    time_elapsed    | 596      |
|    total_timesteps | 396992   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 12       |
|    learning_rate   | 0.011    |
|    step_size       | 4.15e-05 |
---------------------------------
Eval num_timesteps=402000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 402000   |
---------------------------------
Eval num_timesteps=408000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -9.6     |
| time/              |          |
|    total_timesteps | 408000   |
---------------------------------
Eval num_timesteps=414000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 414000   |
---------------------------------
Eval num_timesteps=420000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -9.6     |
| time/              |          |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=426000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 600      |
|    mean_reward     | -6.4     |
| time/              |          |
|    total_timesteps | 426000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 462      |
|    ep_rew_mean     | -5.48    |
|    return_std      | 8.26     |
| time/              |          |
|    fps             | 659      |
|    time_elapsed    | 646      |
|    total_timesteps | 426556   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 13       |
|    learning_rate   | 0.011    |
|    step_size       | 4.17e-05 |
---------------------------------
Eval num_timesteps=432000, episode_reward=-4.60 +/- 9.31
Episode length: 387.00 +/- 173.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 387      |
|    mean_reward     | -4.6     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
Eval num_timesteps=438000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
Eval num_timesteps=444000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 403      |
|    ep_rew_mean     | -4.7     |
|    return_std      | 8.26     |
| time/              |          |
|    fps             | 661      |
|    time_elapsed    | 684      |
|    total_timesteps | 452373   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 14       |
|    learning_rate   | 0.011    |
|    step_size       | 4.17e-05 |
---------------------------------
Eval num_timesteps=456000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=462000, episode_reward=0.00 +/- 0.00
Episode length: 122.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 122      |
|    mean_reward     | 0        |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
New best mean reward!
Eval num_timesteps=468000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=474000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 474000   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 480000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 468      |
|    ep_rew_mean     | -5.69    |
|    return_std      | 8.27     |
| time/              |          |
|    fps             | 665      |
|    time_elapsed    | 724      |
|    total_timesteps | 482357   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 15       |
|    learning_rate   | 0.011    |
|    step_size       | 4.16e-05 |
---------------------------------
Eval num_timesteps=486000, episode_reward=-0.80 +/- 7.60
Episode length: 316.00 +/- 142.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | -0.8     |
| time/              |          |
|    total_timesteps | 486000   |
---------------------------------
Eval num_timesteps=492000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 492000   |
---------------------------------
Eval num_timesteps=498000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 498000   |
---------------------------------
Eval num_timesteps=504000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 504000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 510000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | -5.83    |
|    return_std      | 8.24     |
| time/              |          |
|    fps             | 667      |
|    time_elapsed    | 767      |
|    total_timesteps | 512167   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 16       |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=516000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
Eval num_timesteps=522000, episode_reward=-1.60 +/- 7.20
Episode length: 277.80 +/- 161.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
Eval num_timesteps=528000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
Eval num_timesteps=534000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=540000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 485      |
|    ep_rew_mean     | -6.19    |
|    return_std      | 8.22     |
| time/              |          |
|    fps             | 669      |
|    time_elapsed    | 811      |
|    total_timesteps | 543196   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 17       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=546000, episode_reward=-2.40 +/- 6.86
Episode length: 247.60 +/- 179.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -2.4     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=552000, episode_reward=-2.80 +/- 6.65
Episode length: 232.60 +/- 185.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -2.8     |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=558000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 558000   |
---------------------------------
Eval num_timesteps=564000, episode_reward=-2.40 +/- 6.86
Episode length: 247.60 +/- 179.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -2.4     |
| time/              |          |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-2.80 +/- 6.65
Episode length: 232.60 +/- 185.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -2.8     |
| time/              |          |
|    total_timesteps | 570000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 504      |
|    ep_rew_mean     | -6.22    |
|    return_std      | 8.29     |
| time/              |          |
|    fps             | 672      |
|    time_elapsed    | 855      |
|    total_timesteps | 575468   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 18       |
|    learning_rate   | 0.011    |
|    step_size       | 4.16e-05 |
---------------------------------
Eval num_timesteps=576000, episode_reward=-1.20 +/- 7.41
Episode length: 297.00 +/- 153.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=582000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 582000   |
---------------------------------
Eval num_timesteps=588000, episode_reward=-1.00 +/- 7.51
Episode length: 306.60 +/- 147.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 588000   |
---------------------------------
Eval num_timesteps=594000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 594000   |
---------------------------------
Eval num_timesteps=600000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 434      |
|    ep_rew_mean     | -5.12    |
|    return_std      | 8.16     |
| time/              |          |
|    fps             | 672      |
|    time_elapsed    | 896      |
|    total_timesteps | 603212   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 19       |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=606000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
Eval num_timesteps=612000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
Eval num_timesteps=618000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
Eval num_timesteps=624000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
Eval num_timesteps=630000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
Eval num_timesteps=636000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 522      |
|    ep_rew_mean     | -6.61    |
|    return_std      | 8.3      |
| time/              |          |
|    fps             | 674      |
|    time_elapsed    | 944      |
|    total_timesteps | 636605   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 20       |
|    learning_rate   | 0.011    |
|    step_size       | 4.15e-05 |
---------------------------------
Eval num_timesteps=642000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 642000   |
---------------------------------
Eval num_timesteps=648000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 648000   |
---------------------------------
Eval num_timesteps=654000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 654000   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=666000, episode_reward=-3.20 +/- 6.40
Episode length: 217.80 +/- 191.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 666000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 475      |
|    ep_rew_mean     | -5.77    |
|    return_std      | 8.21     |
| time/              |          |
|    fps             | 676      |
|    time_elapsed    | 985      |
|    total_timesteps | 666994   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 21       |
|    learning_rate   | 0.011    |
|    step_size       | 4.2e-05  |
---------------------------------
Eval num_timesteps=672000, episode_reward=-2.80 +/- 6.65
Episode length: 232.60 +/- 185.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -2.8     |
| time/              |          |
|    total_timesteps | 672000   |
---------------------------------
Eval num_timesteps=678000, episode_reward=-2.40 +/- 6.86
Episode length: 247.60 +/- 179.36
---------------------------------
| eval/              |          |
|    mean_ep_length  | 248      |
|    mean_reward     | -2.4     |
| time/              |          |
|    total_timesteps | 678000   |
---------------------------------
Eval num_timesteps=684000, episode_reward=-3.20 +/- 6.40
Episode length: 217.60 +/- 191.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 218      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
Eval num_timesteps=690000, episode_reward=0.40 +/- 0.80
Episode length: 137.00 +/- 30.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 137      |
|    mean_reward     | 0.4      |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
New best mean reward!
Eval num_timesteps=696000, episode_reward=-2.80 +/- 6.65
Episode length: 232.60 +/- 185.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 233      |
|    mean_reward     | -2.8     |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 463      |
|    ep_rew_mean     | -5.62    |
|    return_std      | 8.36     |
| time/              |          |
|    fps             | 679      |
|    time_elapsed    | 1025     |
|    total_timesteps | 696637   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 22       |
|    learning_rate   | 0.011    |
|    step_size       | 4.12e-05 |
---------------------------------
Eval num_timesteps=702000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
Eval num_timesteps=708000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
Eval num_timesteps=714000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
Eval num_timesteps=720000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=726000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 476      |
|    ep_rew_mean     | -5.61    |
|    return_std      | 8.16     |
| time/              |          |
|    fps             | 679      |
|    time_elapsed    | 1069     |
|    total_timesteps | 727132   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 23       |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=732000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 732000   |
---------------------------------
Eval num_timesteps=738000, episode_reward=-0.60 +/- 5.71
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -0.6     |
| time/              |          |
|    total_timesteps | 738000   |
---------------------------------
Eval num_timesteps=744000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 744000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=-1.20 +/- 6.40
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 750000   |
---------------------------------
Eval num_timesteps=756000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 756000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 466      |
|    ep_rew_mean     | -5.53    |
|    return_std      | 8.29     |
| time/              |          |
|    fps             | 680      |
|    time_elapsed    | 1112     |
|    total_timesteps | 756955   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 24       |
|    learning_rate   | 0.011    |
|    step_size       | 4.16e-05 |
---------------------------------
Eval num_timesteps=762000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 762000   |
---------------------------------
Eval num_timesteps=768000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=774000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
Eval num_timesteps=780000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=786000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 487      |
|    ep_rew_mean     | -5.92    |
|    return_std      | 8.45     |
| time/              |          |
|    fps             | 681      |
|    time_elapsed    | 1156     |
|    total_timesteps | 788093   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 25       |
|    learning_rate   | 0.011    |
|    step_size       | 4.08e-05 |
---------------------------------
Eval num_timesteps=792000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
Eval num_timesteps=798000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
Eval num_timesteps=804000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
Eval num_timesteps=810000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
Eval num_timesteps=816000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 816000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 458      |
|    ep_rew_mean     | -5.61    |
|    return_std      | 8.16     |
| time/              |          |
|    fps             | 681      |
|    time_elapsed    | 1198     |
|    total_timesteps | 817399   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 26       |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=822000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 822000   |
---------------------------------
Eval num_timesteps=828000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 828000   |
---------------------------------
Eval num_timesteps=834000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 834000   |
---------------------------------
Eval num_timesteps=840000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 840000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 434      |
|    ep_rew_mean     | -5       |
|    return_std      | 8.37     |
| time/              |          |
|    fps             | 682      |
|    time_elapsed    | 1237     |
|    total_timesteps | 845179   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 27       |
|    learning_rate   | 0.011    |
|    step_size       | 4.12e-05 |
---------------------------------
Eval num_timesteps=846000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 846000   |
---------------------------------
Eval num_timesteps=852000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 852000   |
---------------------------------
Eval num_timesteps=858000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
Eval num_timesteps=864000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
Eval num_timesteps=870000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 478      |
|    ep_rew_mean     | -6.02    |
|    return_std      | 8.38     |
| time/              |          |
|    fps             | 683      |
|    time_elapsed    | 1281     |
|    total_timesteps | 875739   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 28       |
|    learning_rate   | 0.011    |
|    step_size       | 4.11e-05 |
---------------------------------
Eval num_timesteps=876000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
Eval num_timesteps=882000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
Eval num_timesteps=888000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
Eval num_timesteps=894000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
Eval num_timesteps=900000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 900000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 434      |
|    ep_rew_mean     | -5.16    |
|    return_std      | 8.2      |
| time/              |          |
|    fps             | 683      |
|    time_elapsed    | 1322     |
|    total_timesteps | 903541   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 29       |
|    learning_rate   | 0.011    |
|    step_size       | 4.2e-05  |
---------------------------------
Eval num_timesteps=906000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 906000   |
---------------------------------
Eval num_timesteps=912000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 912000   |
---------------------------------
Eval num_timesteps=918000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 918000   |
---------------------------------
Eval num_timesteps=924000, episode_reward=-0.80 +/- 6.11
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -0.8     |
| time/              |          |
|    total_timesteps | 924000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 930000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 429      |
|    ep_rew_mean     | -5.06    |
|    return_std      | 8.19     |
| time/              |          |
|    fps             | 683      |
|    time_elapsed    | 1362     |
|    total_timesteps | 931014   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 30       |
|    learning_rate   | 0.011    |
|    step_size       | 4.21e-05 |
---------------------------------
Eval num_timesteps=936000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 936000   |
---------------------------------
Eval num_timesteps=942000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
Eval num_timesteps=948000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
Eval num_timesteps=954000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
Eval num_timesteps=960000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 492      |
|    ep_rew_mean     | -5.95    |
|    return_std      | 8.3      |
| time/              |          |
|    fps             | 683      |
|    time_elapsed    | 1407     |
|    total_timesteps | 962482   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 31       |
|    learning_rate   | 0.011    |
|    step_size       | 4.15e-05 |
---------------------------------
Eval num_timesteps=966000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
Eval num_timesteps=972000, episode_reward=-1.20 +/- 6.40
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
Eval num_timesteps=978000, episode_reward=-4.00 +/- 7.77
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -4       |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
Eval num_timesteps=984000, episode_reward=-1.00 +/- 6.51
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 984000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=-1.20 +/- 6.91
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 990000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 461      |
|    ep_rew_mean     | -5.67    |
|    return_std      | 8.35     |
| time/              |          |
|    fps             | 684      |
|    time_elapsed    | 1449     |
|    total_timesteps | 991979   |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 32       |
|    learning_rate   | 0.011    |
|    step_size       | 4.13e-05 |
---------------------------------
Eval num_timesteps=996000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=1002000, episode_reward=-5.00 +/- 8.99
Episode length: 368.00 +/- 190.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1002000  |
---------------------------------
Eval num_timesteps=1008000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1008000  |
---------------------------------
Eval num_timesteps=1014000, episode_reward=-1.40 +/- 7.31
Episode length: 287.40 +/- 157.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1014000  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1020000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 442      |
|    ep_rew_mean     | -5.16    |
|    return_std      | 8.23     |
| time/              |          |
|    fps             | 684      |
|    time_elapsed    | 1491     |
|    total_timesteps | 1020287  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 33       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=1026000, episode_reward=-1.40 +/- 6.80
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1026000  |
---------------------------------
Eval num_timesteps=1032000, episode_reward=-0.20 +/- 4.92
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -0.2     |
| time/              |          |
|    total_timesteps | 1032000  |
---------------------------------
Eval num_timesteps=1038000, episode_reward=-1.20 +/- 6.91
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1038000  |
---------------------------------
Eval num_timesteps=1044000, episode_reward=-4.20 +/- 7.60
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -4.2     |
| time/              |          |
|    total_timesteps | 1044000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 456      |
|    ep_rew_mean     | -5.89    |
|    return_std      | 8.19     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1531     |
|    total_timesteps | 1049442  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 34       |
|    learning_rate   | 0.011    |
|    step_size       | 4.21e-05 |
---------------------------------
Eval num_timesteps=1050000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1050000  |
---------------------------------
Eval num_timesteps=1056000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1056000  |
---------------------------------
Eval num_timesteps=1062000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1062000  |
---------------------------------
Eval num_timesteps=1068000, episode_reward=-1.00 +/- 7.51
Episode length: 306.60 +/- 147.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1068000  |
---------------------------------
Eval num_timesteps=1074000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1074000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 419      |
|    ep_rew_mean     | -4.66    |
|    return_std      | 8.3      |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1571     |
|    total_timesteps | 1076262  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 35       |
|    learning_rate   | 0.011    |
|    step_size       | 4.15e-05 |
---------------------------------
Eval num_timesteps=1080000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1086000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1086000  |
---------------------------------
Eval num_timesteps=1092000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1092000  |
---------------------------------
Eval num_timesteps=1098000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1098000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 371      |
|    ep_rew_mean     | -4.3     |
|    return_std      | 8.01     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1605     |
|    total_timesteps | 1100010  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 36       |
|    learning_rate   | 0.011    |
|    step_size       | 4.3e-05  |
---------------------------------
Eval num_timesteps=1104000, episode_reward=-4.40 +/- 7.84
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -4.4     |
| time/              |          |
|    total_timesteps | 1104000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1110000  |
---------------------------------
Eval num_timesteps=1116000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1116000  |
---------------------------------
Eval num_timesteps=1122000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1122000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 382      |
|    ep_rew_mean     | -4.2     |
|    return_std      | 8.07     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1640     |
|    total_timesteps | 1124456  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 37       |
|    learning_rate   | 0.011    |
|    step_size       | 4.27e-05 |
---------------------------------
Eval num_timesteps=1128000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1128000  |
---------------------------------
Eval num_timesteps=1134000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1134000  |
---------------------------------
Eval num_timesteps=1140000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1146000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1146000  |
---------------------------------
Eval num_timesteps=1152000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1152000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 433      |
|    ep_rew_mean     | -5.38    |
|    return_std      | 7.96     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1681     |
|    total_timesteps | 1152149  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 38       |
|    learning_rate   | 0.011    |
|    step_size       | 4.33e-05 |
---------------------------------
Eval num_timesteps=1158000, episode_reward=-0.20 +/- 4.92
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -0.2     |
| time/              |          |
|    total_timesteps | 1158000  |
---------------------------------
Eval num_timesteps=1164000, episode_reward=-0.40 +/- 4.80
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -0.4     |
| time/              |          |
|    total_timesteps | 1164000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=-4.40 +/- 8.31
Episode length: 368.20 +/- 190.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -4.4     |
| time/              |          |
|    total_timesteps | 1170000  |
---------------------------------
Eval num_timesteps=1176000, episode_reward=0.20 +/- 4.12
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 1176000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 393      |
|    ep_rew_mean     | -4.42    |
|    return_std      | 8.22     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1717     |
|    total_timesteps | 1177275  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 39       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=1182000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1182000  |
---------------------------------
Eval num_timesteps=1188000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1188000  |
---------------------------------
Eval num_timesteps=1194000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1194000  |
---------------------------------
Eval num_timesteps=1200000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 394      |
|    ep_rew_mean     | -4.56    |
|    return_std      | 8.12     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1753     |
|    total_timesteps | 1202499  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 40       |
|    learning_rate   | 0.011    |
|    step_size       | 4.25e-05 |
---------------------------------
Eval num_timesteps=1206000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1206000  |
---------------------------------
Eval num_timesteps=1212000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1212000  |
---------------------------------
Eval num_timesteps=1218000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1218000  |
---------------------------------
Eval num_timesteps=1224000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1224000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 377      |
|    ep_rew_mean     | -4.17    |
|    return_std      | 8.03     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1788     |
|    total_timesteps | 1226614  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 41       |
|    learning_rate   | 0.011    |
|    step_size       | 4.29e-05 |
---------------------------------
Eval num_timesteps=1230000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1230000  |
---------------------------------
Eval num_timesteps=1236000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1236000  |
---------------------------------
Eval num_timesteps=1242000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1242000  |
---------------------------------
Eval num_timesteps=1248000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1248000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | -4.83    |
|    return_std      | 8.11     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 1825     |
|    total_timesteps | 1252216  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 42       |
|    learning_rate   | 0.011    |
|    step_size       | 4.25e-05 |
---------------------------------
Eval num_timesteps=1254000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1254000  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=-1.60 +/- 7.20
Episode length: 277.80 +/- 161.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1266000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1266000  |
---------------------------------
Eval num_timesteps=1272000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1272000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 392      |
|    ep_rew_mean     | -4.11    |
|    return_std      | 8.03     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 1861     |
|    total_timesteps | 1277321  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 43       |
|    learning_rate   | 0.011    |
|    step_size       | 4.29e-05 |
---------------------------------
Eval num_timesteps=1278000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1278000  |
---------------------------------
Eval num_timesteps=1284000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1284000  |
---------------------------------
Eval num_timesteps=1290000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1290000  |
---------------------------------
Eval num_timesteps=1296000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1296000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 366      |
|    ep_rew_mean     | -4.06    |
|    return_std      | 8.1      |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 1895     |
|    total_timesteps | 1300736  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 44       |
|    learning_rate   | 0.011    |
|    step_size       | 4.25e-05 |
---------------------------------
Eval num_timesteps=1302000, episode_reward=-5.00 +/- 8.99
Episode length: 368.00 +/- 190.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1302000  |
---------------------------------
Eval num_timesteps=1308000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1308000  |
---------------------------------
Eval num_timesteps=1314000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1314000  |
---------------------------------
Eval num_timesteps=1320000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | -3.81    |
|    return_std      | 8.05     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1929     |
|    total_timesteps | 1323613  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 45       |
|    learning_rate   | 0.011    |
|    step_size       | 4.28e-05 |
---------------------------------
Eval num_timesteps=1326000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1326000  |
---------------------------------
Eval num_timesteps=1332000, episode_reward=-1.20 +/- 7.41
Episode length: 297.00 +/- 153.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1332000  |
---------------------------------
Eval num_timesteps=1338000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1338000  |
---------------------------------
Eval num_timesteps=1344000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1344000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 364      |
|    ep_rew_mean     | -4.02    |
|    return_std      | 7.9      |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 1963     |
|    total_timesteps | 1346879  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 46       |
|    learning_rate   | 0.011    |
|    step_size       | 4.36e-05 |
---------------------------------
Eval num_timesteps=1350000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1350000  |
---------------------------------
Eval num_timesteps=1356000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1356000  |
---------------------------------
Eval num_timesteps=1362000, episode_reward=-1.20 +/- 7.41
Episode length: 297.00 +/- 153.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1362000  |
---------------------------------
Eval num_timesteps=1368000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1368000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 395      |
|    ep_rew_mean     | -4.48    |
|    return_std      | 8.16     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 2000     |
|    total_timesteps | 1372182  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 47       |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=1374000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1374000  |
---------------------------------
Eval num_timesteps=1380000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1386000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1386000  |
---------------------------------
Eval num_timesteps=1392000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1392000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 376      |
|    ep_rew_mean     | -4.39    |
|    return_std      | 8.23     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 2035     |
|    total_timesteps | 1396277  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 48       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=1398000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1398000  |
---------------------------------
Eval num_timesteps=1404000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1404000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1410000  |
---------------------------------
Eval num_timesteps=1416000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1416000  |
---------------------------------
Eval num_timesteps=1422000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1422000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | -4.95    |
|    return_std      | 8.11     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 2075     |
|    total_timesteps | 1422639  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 49       |
|    learning_rate   | 0.011    |
|    step_size       | 4.25e-05 |
---------------------------------
Eval num_timesteps=1428000, episode_reward=-1.60 +/- 7.20
Episode length: 277.80 +/- 161.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1428000  |
---------------------------------
Eval num_timesteps=1434000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1434000  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1446000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1446000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 400      |
|    ep_rew_mean     | -4.53    |
|    return_std      | 7.97     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 2111     |
|    total_timesteps | 1448252  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 50       |
|    learning_rate   | 0.011    |
|    step_size       | 4.32e-05 |
---------------------------------
Eval num_timesteps=1452000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1452000  |
---------------------------------
Eval num_timesteps=1458000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1458000  |
---------------------------------
Eval num_timesteps=1464000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1464000  |
---------------------------------
Eval num_timesteps=1470000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1470000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 409      |
|    ep_rew_mean     | -4.8     |
|    return_std      | 8.24     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2148     |
|    total_timesteps | 1474411  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 51       |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=1476000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1476000  |
---------------------------------
Eval num_timesteps=1482000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1482000  |
---------------------------------
Eval num_timesteps=1488000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1488000  |
---------------------------------
Eval num_timesteps=1494000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1494000  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1500000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | -4.88    |
|    return_std      | 8.17     |
| time/              |          |
|    fps             | 685      |
|    time_elapsed    | 2187     |
|    total_timesteps | 1500676  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 52       |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=1506000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1506000  |
---------------------------------
Eval num_timesteps=1512000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1512000  |
---------------------------------
Eval num_timesteps=1518000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1518000  |
---------------------------------
Eval num_timesteps=1524000, episode_reward=-1.20 +/- 7.41
Episode length: 297.00 +/- 153.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1524000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | -4.73    |
|    return_std      | 8.28     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2225     |
|    total_timesteps | 1527066  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 53       |
|    learning_rate   | 0.011    |
|    step_size       | 4.16e-05 |
---------------------------------
Eval num_timesteps=1530000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1530000  |
---------------------------------
Eval num_timesteps=1536000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1542000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1542000  |
---------------------------------
Eval num_timesteps=1548000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1548000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 412      |
|    ep_rew_mean     | -4.78    |
|    return_std      | 8.07     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2262     |
|    total_timesteps | 1553427  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 54       |
|    learning_rate   | 0.011    |
|    step_size       | 4.27e-05 |
---------------------------------
Eval num_timesteps=1554000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1554000  |
---------------------------------
Eval num_timesteps=1560000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1566000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1566000  |
---------------------------------
Eval num_timesteps=1572000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1572000  |
---------------------------------
Eval num_timesteps=1578000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1578000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | -4.45    |
|    return_std      | 8.18     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2301     |
|    total_timesteps | 1579345  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 55       |
|    learning_rate   | 0.011    |
|    step_size       | 4.21e-05 |
---------------------------------
Eval num_timesteps=1584000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1584000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1590000  |
---------------------------------
Eval num_timesteps=1596000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1596000  |
---------------------------------
Eval num_timesteps=1602000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1602000  |
---------------------------------
Eval num_timesteps=1608000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1608000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 451      |
|    ep_rew_mean     | -5.38    |
|    return_std      | 8.34     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2343     |
|    total_timesteps | 1608215  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 56       |
|    learning_rate   | 0.011    |
|    step_size       | 4.13e-05 |
---------------------------------
Eval num_timesteps=1614000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1614000  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1626000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1626000  |
---------------------------------
Eval num_timesteps=1632000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1632000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 442      |
|    ep_rew_mean     | -5.42    |
|    return_std      | 8.3      |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2382     |
|    total_timesteps | 1636504  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 57       |
|    learning_rate   | 0.011    |
|    step_size       | 4.15e-05 |
---------------------------------
Eval num_timesteps=1638000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1638000  |
---------------------------------
Eval num_timesteps=1644000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1644000  |
---------------------------------
Eval num_timesteps=1650000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1650000  |
---------------------------------
Eval num_timesteps=1656000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1656000  |
---------------------------------
Eval num_timesteps=1662000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1662000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 405      |
|    ep_rew_mean     | -4.44    |
|    return_std      | 8.2      |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2421     |
|    total_timesteps | 1662445  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 58       |
|    learning_rate   | 0.011    |
|    step_size       | 4.2e-05  |
---------------------------------
Eval num_timesteps=1668000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1668000  |
---------------------------------
Eval num_timesteps=1674000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1674000  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1686000, episode_reward=-4.60 +/- 9.31
Episode length: 387.00 +/- 173.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 387      |
|    mean_reward     | -4.6     |
| time/              |          |
|    total_timesteps | 1686000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 445      |
|    ep_rew_mean     | -5.34    |
|    return_std      | 8.37     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2461     |
|    total_timesteps | 1690895  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 59       |
|    learning_rate   | 0.011    |
|    step_size       | 4.12e-05 |
---------------------------------
Eval num_timesteps=1692000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1692000  |
---------------------------------
Eval num_timesteps=1698000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1698000  |
---------------------------------
Eval num_timesteps=1704000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1704000  |
---------------------------------
Eval num_timesteps=1710000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1710000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 354      |
|    ep_rew_mean     | -3.7     |
|    return_std      | 8.12     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2494     |
|    total_timesteps | 1713542  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 60       |
|    learning_rate   | 0.011    |
|    step_size       | 4.25e-05 |
---------------------------------
Eval num_timesteps=1716000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1716000  |
---------------------------------
Eval num_timesteps=1722000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1722000  |
---------------------------------
Eval num_timesteps=1728000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1728000  |
---------------------------------
Eval num_timesteps=1734000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1734000  |
---------------------------------
Eval num_timesteps=1740000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1740000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 432      |
|    ep_rew_mean     | -5.02    |
|    return_std      | 8.35     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 2535     |
|    total_timesteps | 1741222  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 61       |
|    learning_rate   | 0.011    |
|    step_size       | 4.12e-05 |
---------------------------------
Eval num_timesteps=1746000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1746000  |
---------------------------------
Eval num_timesteps=1752000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1752000  |
---------------------------------
Eval num_timesteps=1758000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1758000  |
---------------------------------
Eval num_timesteps=1764000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1764000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 436      |
|    ep_rew_mean     | -5.03    |
|    return_std      | 8.24     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2574     |
|    total_timesteps | 1769156  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 62       |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=1770000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1770000  |
---------------------------------
Eval num_timesteps=1776000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1776000  |
---------------------------------
Eval num_timesteps=1782000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1782000  |
---------------------------------
Eval num_timesteps=1788000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1788000  |
---------------------------------
Eval num_timesteps=1794000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1794000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 426      |
|    ep_rew_mean     | -5.17    |
|    return_std      | 8.16     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2614     |
|    total_timesteps | 1796388  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 63       |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=1800000, episode_reward=-0.80 +/- 7.60
Episode length: 316.00 +/- 142.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | -0.8     |
| time/              |          |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1806000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1806000  |
---------------------------------
Eval num_timesteps=1812000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1812000  |
---------------------------------
Eval num_timesteps=1818000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1818000  |
---------------------------------
Eval num_timesteps=1824000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1824000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 440      |
|    ep_rew_mean     | -5.09    |
|    return_std      | 8.22     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2655     |
|    total_timesteps | 1824527  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 64       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=1830000, episode_reward=-3.80 +/- 7.55
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -3.8     |
| time/              |          |
|    total_timesteps | 1830000  |
---------------------------------
Eval num_timesteps=1836000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1836000  |
---------------------------------
Eval num_timesteps=1842000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1842000  |
---------------------------------
Eval num_timesteps=1848000, episode_reward=-4.40 +/- 7.84
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -4.4     |
| time/              |          |
|    total_timesteps | 1848000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 410      |
|    ep_rew_mean     | -4.7     |
|    return_std      | 8.22     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2693     |
|    total_timesteps | 1850756  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 65       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=1854000, episode_reward=-0.60 +/- 5.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -0.6     |
| time/              |          |
|    total_timesteps | 1854000  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=-1.20 +/- 6.40
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1866000, episode_reward=0.20 +/- 4.62
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 1866000  |
---------------------------------
Eval num_timesteps=1872000, episode_reward=-3.20 +/- 6.37
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 1872000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 383      |
|    ep_rew_mean     | -4.36    |
|    return_std      | 8.15     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2728     |
|    total_timesteps | 1875274  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 66       |
|    learning_rate   | 0.011    |
|    step_size       | 4.23e-05 |
---------------------------------
Eval num_timesteps=1878000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1878000  |
---------------------------------
Eval num_timesteps=1884000, episode_reward=-1.00 +/- 7.51
Episode length: 306.60 +/- 147.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1884000  |
---------------------------------
Eval num_timesteps=1890000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 1890000  |
---------------------------------
Eval num_timesteps=1896000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1896000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 411      |
|    ep_rew_mean     | -4.5     |
|    return_std      | 8.36     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2766     |
|    total_timesteps | 1901598  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 67       |
|    learning_rate   | 0.011    |
|    step_size       | 4.12e-05 |
---------------------------------
Eval num_timesteps=1902000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1902000  |
---------------------------------
Eval num_timesteps=1908000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 1908000  |
---------------------------------
Eval num_timesteps=1914000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1914000  |
---------------------------------
Eval num_timesteps=1920000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1926000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 1926000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 399      |
|    ep_rew_mean     | -4.38    |
|    return_std      | 8.21     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2804     |
|    total_timesteps | 1927165  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 68       |
|    learning_rate   | 0.011    |
|    step_size       | 4.2e-05  |
---------------------------------
Eval num_timesteps=1932000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1932000  |
---------------------------------
Eval num_timesteps=1938000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 1938000  |
---------------------------------
Eval num_timesteps=1944000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1944000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1950000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 378      |
|    ep_rew_mean     | -3.88    |
|    return_std      | 8.28     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2840     |
|    total_timesteps | 1951346  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 69       |
|    learning_rate   | 0.011    |
|    step_size       | 4.16e-05 |
---------------------------------
Eval num_timesteps=1956000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 1956000  |
---------------------------------
Eval num_timesteps=1962000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1962000  |
---------------------------------
Eval num_timesteps=1968000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1968000  |
---------------------------------
Eval num_timesteps=1974000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1974000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 375      |
|    ep_rew_mean     | -3.75    |
|    return_std      | 8.25     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2875     |
|    total_timesteps | 1975316  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 70       |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=1980000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1986000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 1986000  |
---------------------------------
Eval num_timesteps=1992000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1992000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 331      |
|    ep_rew_mean     | -3.19    |
|    return_std      | 8.01     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2905     |
|    total_timesteps | 1996484  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 71       |
|    learning_rate   | 0.011    |
|    step_size       | 4.3e-05  |
---------------------------------
Eval num_timesteps=1998000, episode_reward=-1.40 +/- 7.31
Episode length: 287.40 +/- 157.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 1998000  |
---------------------------------
Eval num_timesteps=2004000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2004000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=-5.20 +/- 8.82
Episode length: 358.40 +/- 197.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2010000  |
---------------------------------
Eval num_timesteps=2016000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 2016000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 371      |
|    ep_rew_mean     | -3.94    |
|    return_std      | 8.24     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2939     |
|    total_timesteps | 2020212  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 72       |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=2022000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 2022000  |
---------------------------------
Eval num_timesteps=2028000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2028000  |
---------------------------------
Eval num_timesteps=2034000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2034000  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | -3.53    |
|    return_std      | 8.05     |
| time/              |          |
|    fps             | 687      |
|    time_elapsed    | 2973     |
|    total_timesteps | 2043145  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 73       |
|    learning_rate   | 0.011    |
|    step_size       | 4.28e-05 |
---------------------------------
Eval num_timesteps=2046000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2046000  |
---------------------------------
Eval num_timesteps=2052000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2052000  |
---------------------------------
Eval num_timesteps=2058000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2058000  |
---------------------------------
Eval num_timesteps=2064000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2064000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 357      |
|    ep_rew_mean     | -3.48    |
|    return_std      | 8.25     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3007     |
|    total_timesteps | 2065965  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 74       |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=2070000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2070000  |
---------------------------------
Eval num_timesteps=2076000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2076000  |
---------------------------------
Eval num_timesteps=2082000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2082000  |
---------------------------------
Eval num_timesteps=2088000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2088000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 355      |
|    ep_rew_mean     | -3.67    |
|    return_std      | 8.09     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3041     |
|    total_timesteps | 2088714  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 75       |
|    learning_rate   | 0.011    |
|    step_size       | 4.26e-05 |
---------------------------------
Eval num_timesteps=2094000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2094000  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2106000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 2106000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 344      |
|    ep_rew_mean     | -3.36    |
|    return_std      | 8.02     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3072     |
|    total_timesteps | 2110707  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 76       |
|    learning_rate   | 0.011    |
|    step_size       | 4.3e-05  |
---------------------------------
Eval num_timesteps=2112000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2112000  |
---------------------------------
Eval num_timesteps=2118000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2118000  |
---------------------------------
Eval num_timesteps=2124000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2124000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2130000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 351      |
|    ep_rew_mean     | -3.53    |
|    return_std      | 8.14     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3105     |
|    total_timesteps | 2133158  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 77       |
|    learning_rate   | 0.011    |
|    step_size       | 4.23e-05 |
---------------------------------
Eval num_timesteps=2136000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2136000  |
---------------------------------
Eval num_timesteps=2142000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2142000  |
---------------------------------
Eval num_timesteps=2148000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2148000  |
---------------------------------
Eval num_timesteps=2154000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2154000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 347      |
|    ep_rew_mean     | -3.22    |
|    return_std      | 7.92     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3138     |
|    total_timesteps | 2155335  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 78       |
|    learning_rate   | 0.011    |
|    step_size       | 4.35e-05 |
---------------------------------
Eval num_timesteps=2160000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2166000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2166000  |
---------------------------------
Eval num_timesteps=2172000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2172000  |
---------------------------------
Eval num_timesteps=2178000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2178000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 385      |
|    ep_rew_mean     | -4.22    |
|    return_std      | 8.34     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3174     |
|    total_timesteps | 2179974  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 79       |
|    learning_rate   | 0.011    |
|    step_size       | 4.13e-05 |
---------------------------------
Eval num_timesteps=2184000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2184000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2190000  |
---------------------------------
Eval num_timesteps=2196000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2196000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 324      |
|    ep_rew_mean     | -2.7     |
|    return_std      | 7.84     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3203     |
|    total_timesteps | 2200738  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 80       |
|    learning_rate   | 0.011    |
|    step_size       | 4.39e-05 |
---------------------------------
Eval num_timesteps=2202000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2202000  |
---------------------------------
Eval num_timesteps=2208000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2208000  |
---------------------------------
Eval num_timesteps=2214000, episode_reward=-5.00 +/- 8.99
Episode length: 368.00 +/- 190.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2214000  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 327      |
|    ep_rew_mean     | -2.98    |
|    return_std      | 7.93     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3235     |
|    total_timesteps | 2221644  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 81       |
|    learning_rate   | 0.011    |
|    step_size       | 4.34e-05 |
---------------------------------
Eval num_timesteps=2226000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2226000  |
---------------------------------
Eval num_timesteps=2232000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2232000  |
---------------------------------
Eval num_timesteps=2238000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2238000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 333      |
|    ep_rew_mean     | -3.25    |
|    return_std      | 8.09     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3265     |
|    total_timesteps | 2242960  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 82       |
|    learning_rate   | 0.011    |
|    step_size       | 4.26e-05 |
---------------------------------
Eval num_timesteps=2244000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2244000  |
---------------------------------
Eval num_timesteps=2250000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2250000  |
---------------------------------
Eval num_timesteps=2256000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2256000  |
---------------------------------
Eval num_timesteps=2262000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2262000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 348      |
|    ep_rew_mean     | -3.5     |
|    return_std      | 8.23     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3298     |
|    total_timesteps | 2265210  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 83       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=2268000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2268000  |
---------------------------------
Eval num_timesteps=2274000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2274000  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2286000, episode_reward=-1.20 +/- 7.41
Episode length: 297.00 +/- 153.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2286000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 358      |
|    ep_rew_mean     | -3.58    |
|    return_std      | 8.08     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3332     |
|    total_timesteps | 2288134  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 84       |
|    learning_rate   | 0.011    |
|    step_size       | 4.26e-05 |
---------------------------------
Eval num_timesteps=2292000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 2292000  |
---------------------------------
Eval num_timesteps=2298000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2298000  |
---------------------------------
Eval num_timesteps=2304000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2304000  |
---------------------------------
Eval num_timesteps=2310000, episode_reward=-1.40 +/- 7.31
Episode length: 287.40 +/- 157.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2310000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 370      |
|    ep_rew_mean     | -3.95    |
|    return_std      | 8.2      |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3367     |
|    total_timesteps | 2311837  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 85       |
|    learning_rate   | 0.011    |
|    step_size       | 4.2e-05  |
---------------------------------
Eval num_timesteps=2316000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2316000  |
---------------------------------
Eval num_timesteps=2322000, episode_reward=-1.40 +/- 7.31
Episode length: 287.40 +/- 157.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2322000  |
---------------------------------
Eval num_timesteps=2328000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2328000  |
---------------------------------
Eval num_timesteps=2334000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2334000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 379      |
|    ep_rew_mean     | -3.81    |
|    return_std      | 8.16     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3402     |
|    total_timesteps | 2336092  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 86       |
|    learning_rate   | 0.011    |
|    step_size       | 4.22e-05 |
---------------------------------
Eval num_timesteps=2340000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2346000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2346000  |
---------------------------------
Eval num_timesteps=2352000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2352000  |
---------------------------------
Eval num_timesteps=2358000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 2358000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 391      |
|    ep_rew_mean     | -4.19    |
|    return_std      | 8.36     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3438     |
|    total_timesteps | 2361132  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 87       |
|    learning_rate   | 0.011    |
|    step_size       | 4.12e-05 |
---------------------------------
Eval num_timesteps=2364000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2364000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=-1.20 +/- 7.41
Episode length: 296.80 +/- 153.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 297      |
|    mean_reward     | -1.2     |
| time/              |          |
|    total_timesteps | 2370000  |
---------------------------------
Eval num_timesteps=2376000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2376000  |
---------------------------------
Eval num_timesteps=2382000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2382000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 363      |
|    ep_rew_mean     | -3.48    |
|    return_std      | 8.22     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3472     |
|    total_timesteps | 2384382  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 88       |
|    learning_rate   | 0.011    |
|    step_size       | 4.19e-05 |
---------------------------------
Eval num_timesteps=2388000, episode_reward=-1.00 +/- 7.51
Episode length: 306.60 +/- 147.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 307      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 2388000  |
---------------------------------
Eval num_timesteps=2394000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 2394000  |
---------------------------------
Eval num_timesteps=2400000, episode_reward=-1.00 +/- 7.51
Episode length: 306.40 +/- 147.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 306      |
|    mean_reward     | -1       |
| time/              |          |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2406000, episode_reward=-1.60 +/- 7.20
Episode length: 277.60 +/- 161.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2406000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 378      |
|    ep_rew_mean     | -3.84    |
|    return_std      | 8.12     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3507     |
|    total_timesteps | 2408559  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 89       |
|    learning_rate   | 0.011    |
|    step_size       | 4.24e-05 |
---------------------------------
Eval num_timesteps=2412000, episode_reward=-5.20 +/- 8.82
Episode length: 358.20 +/- 197.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 358      |
|    mean_reward     | -5.2     |
| time/              |          |
|    total_timesteps | 2412000  |
---------------------------------
Eval num_timesteps=2418000, episode_reward=-1.60 +/- 7.20
Episode length: 277.80 +/- 161.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 278      |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 2418000  |
---------------------------------
Eval num_timesteps=2424000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2424000  |
---------------------------------
Eval num_timesteps=2430000, episode_reward=-1.40 +/- 7.31
Episode length: 287.40 +/- 157.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2430000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 387      |
|    ep_rew_mean     | -4.31    |
|    return_std      | 8.25     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3543     |
|    total_timesteps | 2433322  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 90       |
|    learning_rate   | 0.011    |
|    step_size       | 4.18e-05 |
---------------------------------
Eval num_timesteps=2436000, episode_reward=-4.80 +/- 9.15
Episode length: 377.40 +/- 182.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 377      |
|    mean_reward     | -4.8     |
| time/              |          |
|    total_timesteps | 2436000  |
---------------------------------
Eval num_timesteps=2442000, episode_reward=-0.80 +/- 7.60
Episode length: 316.20 +/- 141.90
---------------------------------
| eval/              |          |
|    mean_ep_length  | 316      |
|    mean_reward     | -0.8     |
| time/              |          |
|    total_timesteps | 2442000  |
---------------------------------
Eval num_timesteps=2448000, episode_reward=-5.00 +/- 8.99
Episode length: 367.80 +/- 190.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 368      |
|    mean_reward     | -5       |
| time/              |          |
|    total_timesteps | 2448000  |
---------------------------------
Eval num_timesteps=2454000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2454000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 362      |
|    ep_rew_mean     | -3.7     |
|    return_std      | 8.11     |
| time/              |          |
|    fps             | 686      |
|    time_elapsed    | 3578     |
|    total_timesteps | 2456476  |
| train/             |          |
|    delta_std       | 0.025    |
|    iterations      | 91       |
|    learning_rate   | 0.011    |
|    step_size       | 4.25e-05 |
---------------------------------
Eval num_timesteps=2460000, episode_reward=-1.40 +/- 7.31
Episode length: 287.20 +/- 157.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 287      |
|    mean_reward     | -1.4     |
| time/              |          |
|    total_timesteps | 2460000  |
---------------------------------
