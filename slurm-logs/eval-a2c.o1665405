/work/09894/nicolas_/ls6/rl-final-project
Sun Apr 28 18:35:54 CDT 2024
/work/09894/nicolas_/ls6/miniconda3/envs/rl-final/bin:/work/09894/nicolas_/ls6/miniconda3/condabin:/work/09894/nicolas_/ls6/rl-final-project/google-cloud-sdk/bin:/opt/apps/xalt/xalt/bin:/opt/apps/pmix/3.2.3/bin:/opt/apps/cmake/3.24.2/bin:/opt/apps/intel19/python3/3.9.7/bin:/opt/apps/autotools/1.4/bin:/opt/intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/opt/intel/compilers_and_libraries_2020.1.217/linux/bin/intel64:/opt/apps/gcc/9.4.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:.
========== multitask-atari ==========
Seed: 1016717774
Loading hyperparameters from: /work/09894/nicolas_/ls6/miniconda3/envs/rl-final/lib/python3.10/site-packages/rl_zoo3/hyperparams/a2c.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('ent_coef', 2.4772847159542594e-06),
             ('gae_lambda', 0.8),
             ('gamma', 0.95),
             ('learning_rate', 0.2639483765900524),
             ('max_grad_norm', 1),
             ('n_steps', 256),
             ('n_timesteps', 10000000.0),
             ('normalize_advantage', False),
             ('policy', 'CnnPolicy'),
             ('policy_kwargs',
              'dict(ortho_init=False, '
              'net_arch=dict(pi=[256,256],vf=[256,256]), '
              'activation_fn=nn.ReLU)'),
             ('use_rms_prop', True),
             ('vf_coef', 0.1990617811319626)])
Using 1 environments
Creating test environment
Using cpu device
Log path: logs/a2c/multitask-atari_18
Eval num_timesteps=6000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 6000      |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 23        |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
New best mean reward!
Eval num_timesteps=12000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 12000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.47    |
|    learning_rate      | 0.264    |
|    n_updates          | 46       |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=18000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 18000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 70        |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=24000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 24000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 93        |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 589      |
|    ep_rew_mean        | -7.81    |
| time/                 |          |
|    fps                | 281      |
|    iterations         | 100      |
|    time_elapsed       | 90       |
|    total_timesteps    | 25600    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 99       |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=30000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 30000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 117       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=36000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 36000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 140       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=42000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 42000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 164       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=48000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 48000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 187       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 594      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 200      |
|    time_elapsed       | 179      |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0132  |
|    learning_rate      | 0.264    |
|    n_updates          | 199      |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=54000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 54000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.693    |
|    learning_rate      | 0.264    |
|    n_updates          | 210      |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
Eval num_timesteps=60000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 60000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 234       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=66000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 66000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 257       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=72000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 72000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 281       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 285      |
|    iterations         | 300      |
|    time_elapsed       | 268      |
|    total_timesteps    | 76800    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 299      |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=78000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 78000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 304       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=84000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 84000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 328       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=90000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 90000     |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 351       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=96000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 96000    |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 374      |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=102000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 102000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 398       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 282      |
|    iterations         | 400      |
|    time_elapsed       | 362      |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 399      |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=108000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 108000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 421      |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=114000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 114000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 445       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=120000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 120000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 468       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=126000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 126000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 492       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 283      |
|    iterations         | 500      |
|    time_elapsed       | 451      |
|    total_timesteps    | 128000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0142  |
|    learning_rate      | 0.264    |
|    n_updates          | 499      |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=132000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 132000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 515       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=138000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 138000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 539       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=144000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 144000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 562       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=150000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 150000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.692    |
|    learning_rate      | 0.264    |
|    n_updates          | 585      |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 600      |
|    time_elapsed       | 540      |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 599      |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=156000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 156000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 609       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=162000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 162000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 632       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=168000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 168000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 656       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=174000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 174000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 679       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 700      |
|    time_elapsed       | 628      |
|    total_timesteps    | 179200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 699      |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=180000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 180000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 703       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=186000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 186000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 726       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=192000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 192000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 749      |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=198000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 198000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 773       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=204000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 204000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 796      |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 283      |
|    iterations         | 800      |
|    time_elapsed       | 722      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0134  |
|    learning_rate      | 0.264    |
|    n_updates          | 799      |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=210000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 210000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 820       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=216000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 216000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 843       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=222000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 222000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 867       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=228000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 228000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 890       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 283      |
|    iterations         | 900      |
|    time_elapsed       | 811      |
|    total_timesteps    | 230400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 899      |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=234000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 234000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 914       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=240000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 240000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 937       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=246000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 246000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.693    |
|    learning_rate      | 0.264    |
|    n_updates          | 960      |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
Eval num_timesteps=252000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 252000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 984       |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 1000     |
|    time_elapsed       | 900      |
|    total_timesteps    | 256000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 999      |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=258000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 258000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1007      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=264000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 264000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1031      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=270000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 270000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1054      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=276000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 276000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1078      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 1100     |
|    time_elapsed       | 988      |
|    total_timesteps    | 281600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0154  |
|    learning_rate      | 0.264    |
|    n_updates          | 1099     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=282000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 282000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1101      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=288000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 288000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 1124     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=294000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 294000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1148      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=300000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 300000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 1171     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=306000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 306000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1195      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 283      |
|    iterations         | 1200     |
|    time_elapsed       | 1082     |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 1199     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=312000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 312000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1218      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=318000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 318000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1242      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=324000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 324000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1265      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=330000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 330000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1289      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 1300     |
|    time_elapsed       | 1171     |
|    total_timesteps    | 332800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 1299     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=336000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 336000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1312      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=342000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 342000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.692    |
|    learning_rate      | 0.264    |
|    n_updates          | 1335     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
Eval num_timesteps=348000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 348000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1359      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=354000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 354000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1382      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 1400     |
|    time_elapsed       | 1259     |
|    total_timesteps    | 358400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0111  |
|    learning_rate      | 0.264    |
|    n_updates          | 1399     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=360000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 360000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1406      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=366000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 366000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1429      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=372000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 372000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1453      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=378000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 378000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1476      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=384000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 384000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 1499     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8       |
| time/              |          |
|    fps             | 283      |
|    iterations      | 1500     |
|    time_elapsed    | 1353     |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 390000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1523      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=396000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 396000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 1546     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=402000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 402000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1570      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=408000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 408000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1593      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 283      |
|    iterations         | 1600     |
|    time_elapsed       | 1442     |
|    total_timesteps    | 409600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.727    |
|    learning_rate      | 0.264    |
|    n_updates          | 1599     |
|    policy_loss        | -0       |
|    value_loss         | 2.86e+20 |
------------------------------------
Eval num_timesteps=414000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 414000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1617      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=420000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 420000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1640      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=426000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 426000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1664      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=432000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 432000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1687      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 1700     |
|    time_elapsed       | 1531     |
|    total_timesteps    | 435200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0137  |
|    learning_rate      | 0.264    |
|    n_updates          | 1699     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=438000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 438000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.692    |
|    learning_rate      | 0.264    |
|    n_updates          | 1710     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
Eval num_timesteps=444000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 444000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1734      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=450000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 450000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1757      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=456000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 456000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1781      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 1800     |
|    time_elapsed       | 1619     |
|    total_timesteps    | 460800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 1799     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=462000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 462000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1804      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=468000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 468000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1828      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=474000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 474000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1851      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=480000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 480000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 1874     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=486000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 486000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1898      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 283      |
|    iterations         | 1900     |
|    time_elapsed       | 1713     |
|    total_timesteps    | 486400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 1899     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=492000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 492000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.47    |
|    learning_rate      | 0.264    |
|    n_updates          | 1921     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=498000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 498000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1945      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=504000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 504000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1968      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=510000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 510000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 1992      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2000     |
|    time_elapsed       | 1801     |
|    total_timesteps    | 512000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0113  |
|    learning_rate      | 0.264    |
|    n_updates          | 1999     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=516000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 516000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2015      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=522000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 522000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2039      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=528000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 528000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2062      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=534000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 534000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.692    |
|    learning_rate      | 0.264    |
|    n_updates          | 2085     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2100     |
|    time_elapsed       | 1890     |
|    total_timesteps    | 537600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 2099     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=540000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 540000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2109      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=546000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 546000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2132      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=552000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 552000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2156      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=558000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 558000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2179      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2200     |
|    time_elapsed       | 1979     |
|    total_timesteps    | 563200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 2199     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=564000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 564000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2203      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=570000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 570000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2226      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=576000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 576000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 2249     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=582000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 582000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2273      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=588000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 588000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 2296     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2300     |
|    time_elapsed       | 2072     |
|    total_timesteps    | 588800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0142  |
|    learning_rate      | 0.264    |
|    n_updates          | 2299     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=594000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 594000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2320      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=600000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 600000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2343      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=606000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 606000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2367      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=612000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 612000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2390      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2400     |
|    time_elapsed       | 2161     |
|    total_timesteps    | 614400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 2399     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=618000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 618000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2414      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=624000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 624000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2437      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=630000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 630000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.692    |
|    learning_rate      | 0.264    |
|    n_updates          | 2460     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
Eval num_timesteps=636000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 636000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2484      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2500     |
|    time_elapsed       | 2250     |
|    total_timesteps    | 640000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 2499     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=642000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 642000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2507      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=648000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 648000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2531      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=654000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 654000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2554      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=660000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 660000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2578      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2600     |
|    time_elapsed       | 2338     |
|    total_timesteps    | 665600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0113  |
|    learning_rate      | 0.264    |
|    n_updates          | 2599     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=666000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 666000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2601      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=672000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 672000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 2624     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=678000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 678000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2648      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=684000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 684000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.47    |
|    learning_rate      | 0.264    |
|    n_updates          | 2671     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=690000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 690000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2695      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2700     |
|    time_elapsed       | 2432     |
|    total_timesteps    | 691200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 2699     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=696000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 696000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2718      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=702000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 702000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2742      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=708000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 708000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2765      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=714000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 714000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2789      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2800     |
|    time_elapsed       | 2521     |
|    total_timesteps    | 716800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 2799     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=720000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 720000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2812      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=726000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 726000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.692    |
|    learning_rate      | 0.264    |
|    n_updates          | 2835     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
Eval num_timesteps=732000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 732000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2859      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=738000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 738000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2882      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 2900     |
|    time_elapsed       | 2609     |
|    total_timesteps    | 742400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0136  |
|    learning_rate      | 0.264    |
|    n_updates          | 2899     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=744000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 744000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2906      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=750000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 750000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2929      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=756000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 756000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2953      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=762000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 762000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 2976      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=768000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 768000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 2999     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 600      |
|    ep_rew_mean     | -8       |
| time/              |          |
|    fps             | 284      |
|    iterations      | 3000     |
|    time_elapsed    | 2703     |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=774000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 774000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3023      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=780000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 780000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 3046     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=786000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 786000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3070      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=792000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 792000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3093      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3100     |
|    time_elapsed       | 2792     |
|    total_timesteps    | 793600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 3099     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=798000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 798000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3117      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=804000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 804000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3140      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=810000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 810000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3164      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=816000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 816000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3187      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3200     |
|    time_elapsed       | 2880     |
|    total_timesteps    | 819200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0143  |
|    learning_rate      | 0.264    |
|    n_updates          | 3199     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=822000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 822000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.693    |
|    learning_rate      | 0.264    |
|    n_updates          | 3210     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
Eval num_timesteps=828000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 828000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3234      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=834000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 834000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3257      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=840000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 840000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3281      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3300     |
|    time_elapsed       | 2969     |
|    total_timesteps    | 844800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 3299     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=846000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 846000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3304      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=852000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 852000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3328      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=858000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 858000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3351      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=864000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 864000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 3374     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=870000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 870000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3398      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3400     |
|    time_elapsed       | 3063     |
|    total_timesteps    | 870400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 3399     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=876000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 876000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 3421     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
Eval num_timesteps=882000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 882000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3445      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=888000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 888000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3468      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=894000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 894000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3492      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3500     |
|    time_elapsed       | 3152     |
|    total_timesteps    | 896000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.013   |
|    learning_rate      | 0.264    |
|    n_updates          | 3499     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=900000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 900000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3515      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=906000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 906000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3539      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=912000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 912000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3562      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=918000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 918000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.692    |
|    learning_rate      | 0.264    |
|    n_updates          | 3585     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3600     |
|    time_elapsed       | 3240     |
|    total_timesteps    | 921600   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 3599     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=924000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 924000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3609      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=930000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 930000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3632      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=936000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 936000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3656      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=942000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 942000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3679      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3700     |
|    time_elapsed       | 3329     |
|    total_timesteps    | 947200   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.777    |
|    learning_rate      | 0.264    |
|    n_updates          | 3699     |
|    policy_loss        | -0       |
|    value_loss         | 2.62e+20 |
------------------------------------
Eval num_timesteps=948000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 948000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3703      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=954000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 954000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3726      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=960000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 960000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 3749     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=966000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 966000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3773      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=972000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -9.6     |
| time/                 |          |
|    total_timesteps    | 972000   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.469   |
|    learning_rate      | 0.264    |
|    n_updates          | 3796     |
|    policy_loss        | -0       |
|    value_loss         | 2.91e+20 |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3800     |
|    time_elapsed       | 3423     |
|    total_timesteps    | 972800   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | -0.0134  |
|    learning_rate      | 0.264    |
|    n_updates          | 3799     |
|    policy_loss        | -0       |
|    value_loss         | 1.04e+21 |
------------------------------------
Eval num_timesteps=978000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 978000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3820      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=984000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 984000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3843      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=990000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 990000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3867      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=996000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 996000    |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3890      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 600      |
|    ep_rew_mean        | -8       |
| time/                 |          |
|    fps                | 284      |
|    iterations         | 3900     |
|    time_elapsed       | 3512     |
|    total_timesteps    | 998400   |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.801    |
|    learning_rate      | 0.264    |
|    n_updates          | 3899     |
|    policy_loss        | -0       |
|    value_loss         | 4.12e+20 |
------------------------------------
Eval num_timesteps=1002000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -6.4      |
| time/                 |           |
|    total_timesteps    | 1002000   |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3914      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=1008000, episode_reward=-9.60 +/- 7.84
Episode length: 600.00 +/- 0.00
-------------------------------------
| eval/                 |           |
|    mean_ep_length     | 600       |
|    mean_reward        | -9.6      |
| time/                 |           |
|    total_timesteps    | 1008000   |
| train/                |           |
|    entropy_loss       | -0        |
|    explained_variance | -1.19e-07 |
|    learning_rate      | 0.264     |
|    n_updates          | 3937      |
|    policy_loss        | -0        |
|    value_loss         | 1.75e+20  |
-------------------------------------
Eval num_timesteps=1014000, episode_reward=-6.40 +/- 7.84
Episode length: 600.00 +/- 0.00
------------------------------------
| eval/                 |          |
|    mean_ep_length     | 600      |
|    mean_reward        | -6.4     |
| time/                 |          |
|    total_timesteps    | 1014000  |
| train/                |          |
|    entropy_loss       | -0       |
|    explained_variance | 0.693    |
|    learning_rate      | 0.264    |
|    n_updates          | 3960     |
|    policy_loss        | -0       |
|    value_loss         | 3.56e+20 |
------------------------------------
